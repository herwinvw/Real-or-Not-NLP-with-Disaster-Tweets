{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model\n",
    "Putting together an ensemble of the average wordvec model and a model using the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_train = pd.read_pickle('train_wordvec.pickle')\n",
    "wordvec_test = pd.read_pickle('test_wordvec.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>wordvec</th>\n",
       "      <th>keyword_wordvec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[-0.26623327, 0.05843069, -0.1404636, -0.05265...</td>\n",
       "      <td>[-0.26623327, 0.05843069, -0.1404636, -0.05265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[-0.025449565, 0.031005142, -0.15566371, -0.23...</td>\n",
       "      <td>[-0.025449565, 0.031005142, -0.15566371, -0.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[0.0059339865, 0.016337818, -0.105279535, -0.0...</td>\n",
       "      <td>[0.0059339865, 0.016337818, -0.105279535, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>[-0.18147185, 0.20731743, 0.014147284, -0.2182...</td>\n",
       "      <td>[-0.18147185, 0.20731743, 0.014147284, -0.2182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[-0.06394094, -0.01423019, 0.0063574947, 0.071...</td>\n",
       "      <td>[-0.06394094, -0.01423019, 0.0063574947, 0.071...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1       1             Forest fire near La Ronge Sask. Canada   \n",
       "2       1  All residents asked to 'shelter in place' are ...   \n",
       "3       1  people receive wildfires evacuation orders in ...   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                             wordvec  \\\n",
       "0  [-0.26623327, 0.05843069, -0.1404636, -0.05265...   \n",
       "1  [-0.025449565, 0.031005142, -0.15566371, -0.23...   \n",
       "2  [0.0059339865, 0.016337818, -0.105279535, -0.0...   \n",
       "3  [-0.18147185, 0.20731743, 0.014147284, -0.2182...   \n",
       "4  [-0.06394094, -0.01423019, 0.0063574947, 0.071...   \n",
       "\n",
       "                                     keyword_wordvec  \n",
       "0  [-0.26623327, 0.05843069, -0.1404636, -0.05265...  \n",
       "1  [-0.025449565, 0.031005142, -0.15566371, -0.23...  \n",
       "2  [0.0059339865, 0.016337818, -0.105279535, -0.0...  \n",
       "3  [-0.18147185, 0.20731743, 0.014147284, -0.2182...  \n",
       "4  [-0.06394094, -0.01423019, 0.0063574947, 0.071...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(wordvec_train, on=['id'])\n",
    "test = test.merge(wordvec_test, on=['id'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_char_n</th>\n",
       "      <th>num_words_n</th>\n",
       "      <th>num_hash_n</th>\n",
       "      <th>num_mention_n</th>\n",
       "      <th>num_url_n</th>\n",
       "      <th>has_location</th>\n",
       "      <th>geocoded</th>\n",
       "      <th>longitude_n</th>\n",
       "      <th>latitude_n</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.433121</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.242038</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.356688</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.541401</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  num_char_n  num_words_n  num_hash_n  num_mention_n  num_url_n  \\\n",
       "0   1    0.433121     0.419355    0.076923            0.0        0.0   \n",
       "1   4    0.242038     0.225806    0.000000            0.0        0.0   \n",
       "2   5    0.847134     0.709677    0.000000            0.0        0.0   \n",
       "3   6    0.356688     0.225806    0.076923            0.0        0.0   \n",
       "4   7    0.541401     0.516129    0.153846            0.0        0.0   \n",
       "\n",
       "   has_location  geocoded  longitude_n  latitude_n  target  \n",
       "0         False     False          0.0         0.0       1  \n",
       "1         False     False          0.0         0.0       1  \n",
       "2         False     False          0.0         0.0       1  \n",
       "3         False     False          0.0         0.0       1  \n",
       "4         False     False          0.0         0.0       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tree = pd.read_csv('train_metafeatures_tree.csv')\n",
    "test_tree = pd.read_csv('test_metafeatures_tree.csv')\n",
    "train_normalized = pd.read_csv('train_metafeatures_normalized.csv')\n",
    "test_normalized = pd.read_csv('test_metafeatures_normalized.csv')\n",
    "train_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_char_n</th>\n",
       "      <th>num_words_n</th>\n",
       "      <th>num_hash_n</th>\n",
       "      <th>num_mention_n</th>\n",
       "      <th>num_url_n</th>\n",
       "      <th>has_location</th>\n",
       "      <th>geocoded</th>\n",
       "      <th>longitude_n</th>\n",
       "      <th>latitude_n</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>wordvec</th>\n",
       "      <th>keyword_wordvec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.433121</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[-0.26623327, 0.05843069, -0.1404636, -0.05265...</td>\n",
       "      <td>[-0.26623327, 0.05843069, -0.1404636, -0.05265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.242038</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[-0.025449565, 0.031005142, -0.15566371, -0.23...</td>\n",
       "      <td>[-0.025449565, 0.031005142, -0.15566371, -0.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[0.0059339865, 0.016337818, -0.105279535, -0.0...</td>\n",
       "      <td>[0.0059339865, 0.016337818, -0.105279535, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.356688</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>[-0.18147185, 0.20731743, 0.014147284, -0.2182...</td>\n",
       "      <td>[-0.18147185, 0.20731743, 0.014147284, -0.2182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.541401</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[-0.06394094, -0.01423019, 0.0063574947, 0.071...</td>\n",
       "      <td>[-0.06394094, -0.01423019, 0.0063574947, 0.071...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  num_char_n  num_words_n  num_hash_n  num_mention_n  num_url_n  \\\n",
       "0   1    0.433121     0.419355    0.076923            0.0        0.0   \n",
       "1   4    0.242038     0.225806    0.000000            0.0        0.0   \n",
       "2   5    0.847134     0.709677    0.000000            0.0        0.0   \n",
       "3   6    0.356688     0.225806    0.076923            0.0        0.0   \n",
       "4   7    0.541401     0.516129    0.153846            0.0        0.0   \n",
       "\n",
       "   has_location  geocoded  longitude_n  latitude_n  target keyword location  \\\n",
       "0         False     False          0.0         0.0       1     NaN      NaN   \n",
       "1         False     False          0.0         0.0       1     NaN      NaN   \n",
       "2         False     False          0.0         0.0       1     NaN      NaN   \n",
       "3         False     False          0.0         0.0       1     NaN      NaN   \n",
       "4         False     False          0.0         0.0       1     NaN      NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1             Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are ...   \n",
       "3  13,000 people receive #wildfires evacuation or...   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1             Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are ...   \n",
       "3  people receive wildfires evacuation orders in ...   \n",
       "4  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                             wordvec  \\\n",
       "0  [-0.26623327, 0.05843069, -0.1404636, -0.05265...   \n",
       "1  [-0.025449565, 0.031005142, -0.15566371, -0.23...   \n",
       "2  [0.0059339865, 0.016337818, -0.105279535, -0.0...   \n",
       "3  [-0.18147185, 0.20731743, 0.014147284, -0.2182...   \n",
       "4  [-0.06394094, -0.01423019, 0.0063574947, 0.071...   \n",
       "\n",
       "                                     keyword_wordvec  \n",
       "0  [-0.26623327, 0.05843069, -0.1404636, -0.05265...  \n",
       "1  [-0.025449565, 0.031005142, -0.15566371, -0.23...  \n",
       "2  [0.0059339865, 0.016337818, -0.105279535, -0.0...  \n",
       "3  [-0.18147185, 0.20731743, 0.014147284, -0.2182...  \n",
       "4  [-0.06394094, -0.01423019, 0.0063574947, 0.071...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tree = train_tree.merge(train, on=['id','target'])\n",
    "train_normalized = train_normalized.merge(train, on=['id','target'])\n",
    "test_tree = test_tree.merge(test, on=['id'])\n",
    "test_normalized = test_normalized.merge(test, on = ['id'])\n",
    "train_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all data in one SVM model\n",
    "Puts metafeatures and features in one big SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'num_char_n', 'num_words_n', 'num_hash_n', 'num_mention_n',\n",
       "       'num_url_n', 'has_location', 'geocoded', 'longitude_n', 'latitude_n',\n",
       "       'target', 'keyword', 'location', 'text', 'cleaned_text', 'wordvec',\n",
       "       'keyword_wordvec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_normalized.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def get_X(df, col_wv, cols_ext):\n",
    "    X = []\n",
    "    for index, row in df.iterrows():\n",
    "        x = row[col_wv]\n",
    "        x = numpy.append(row[cols_ext].values, x)\n",
    "        X.append(x)        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_ext = ['num_char_n', 'num_words_n', 'num_hash_n', 'num_mention_n',\n",
    "       'num_url_n', 'has_location', 'geocoded', 'longitude_n', 'latitude_n']\n",
    "X = get_X(train_normalized, 'wordvec',cols_ext)\n",
    "y = train_normalized['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svm = SVC(kernel=\"rbf\")\n",
    "params = {'C': [0.05, 0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 4]}\n",
    "clf = GridSearchCV(svm, params, scoring=\"f1\", verbose=1, n_jobs=-2, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([158.19189024, 189.40911245, 242.12806582, 265.95231915,\n",
       "        270.01944232]),\n",
       " 'score_time': array([2.1467371 , 2.26172423, 3.16707468, 3.3029592 , 3.25709343]),\n",
       " 'test_score': array([0.76399027, 0.74728941, 0.7749577 , 0.76101695, 0.77564637]),\n",
       " 'train_score': array([0.82279274, 0.79297365, 0.81854249, 0.78786602, 0.79731149])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(clf, X, y, cv=cv, return_train_score=True, scoring='f1')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7645801390849052"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slight improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(model, X, y, X_test, name):\n",
    "    fit = model.fit(X,y)\n",
    "    pred = model.predict(X_test)\n",
    "    submission = pd.DataFrame({\"id\":test['id'], \"target\":pred})\n",
    "    submission.to_csv(name+'.csv', index=False)\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_X(test_normalized, 'wordvec',cols_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  3.8min finished\n"
     ]
    }
   ],
   "source": [
    "fit = prepare_submission(clf, X, y, X_test, 'avg_wordvec_meta_svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.5}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a public score improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking an average wordvecs model and and a metafeatures model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnpackWordvecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def transform(self, df, **transform_params):\n",
    "        X = []\n",
    "        for index, row in df.iterrows():\n",
    "            x = row['wordvec']\n",
    "            #print(len(x))\n",
    "            X.append(x)    \n",
    "        return numpy.array(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_ext = ['num_char', 'num_words', 'num_hash', 'num_mention', 'num_url',\n",
    "            'has_location', 'geocoded', 'longitude_t', 'latitude_t']\n",
    "t = ColumnTransformer(transformers=[('identity', 'passthrough', cols_ext)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth': [1,2,3,4], 'n_estimators':[5,10,20,40,80]}\n",
    "lg = LGBMClassifier(n_jobs=-2, random_state=42, max_depth=1, n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_meta = Pipeline(steps=[('sel', t),\n",
    "                           ('rf', lg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('identity', 'passthrough',\n",
       "                                 ['num_char', 'num_words', 'num_hash',\n",
       "                                  'num_mention', 'num_url', 'has_location',\n",
       "                                  'geocoded', 'longitude_t', 'latitude_t'])])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.fit(train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "twordvec = ColumnTransformer(transformers=[('wv', UnpackWordvecTransformer(), ['wordvec'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('wv', UnpackWordvecTransformer(),\n",
       "                                 ['wordvec'])])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twordvec.fit(train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=42, max_iter=2000, hidden_layer_sizes=(2,), learning_rate_init=5e-05)\n",
    "pipe_nlp = Pipeline(steps=[('sel',twordvec),\n",
    "                           ('mlp',mlp)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "sc = StackingClassifier(estimators=[('meta',pipe_meta), ('nlp',pipe_nlp)], final_estimator=lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1196.22317004, 1014.42060447, 1249.54983878, 1115.1602962 ,\n",
       "        1027.11536098]),\n",
       " 'score_time': array([0.72195482, 0.4821713 , 1.53263855, 0.70491838, 0.69602585]),\n",
       " 'test_score': array([0.76456504, 0.75701684, 0.76960388, 0.769107  , 0.76812749]),\n",
       " 'train_score': array([0.86564763, 0.86231884, 0.81956088, 0.86046979, 0.79418211])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_validate(sc, train_tree, train_tree['target'], cv=cv, return_train_score=True, scoring='f1', n_jobs=-2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7656840507805377"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_tree.drop(columns=['target','id'])\n",
    "X_test = test_tree.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   40.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   39.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   41.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   42.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   52.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('meta',\n",
       "                                Pipeline(steps=[('sel',\n",
       "                                                 ColumnTransformer(transformers=[('wv',\n",
       "                                                                                  UnpackWordvecTransformer(),\n",
       "                                                                                  ['wordvec'])])),\n",
       "                                                ('rf',\n",
       "                                                 GridSearchCV(cv=5,\n",
       "                                                              estimator=LGBMClassifier(n_jobs=-2,\n",
       "                                                                                       random_state=42),\n",
       "                                                              n_jobs=-2,\n",
       "                                                              param_grid={'max_depth': [1,\n",
       "                                                                                        2,\n",
       "                                                                                        3,\n",
       "                                                                                        4],\n",
       "                                                                          'n_estimators': [5,\n",
       "                                                                                           10,\n",
       "                                                                                           20,\n",
       "                                                                                           40,\n",
       "                                                                                           80]},\n",
       "                                                              scoring='f1',\n",
       "                                                              verbose=1))])),\n",
       "                               ('nlp',\n",
       "                                Pipeline(steps=[('sel',\n",
       "                                                 ColumnTransformer(transformers=[('wv',\n",
       "                                                                                  UnpackWordvecTransformer(),\n",
       "                                                                                  ['wordvec'])])),\n",
       "                                                ('mlp',\n",
       "                                                 MLPClassifier(hidden_layer_sizes=(2,),\n",
       "                                                               learning_rate_init=5e-05,\n",
       "                                                               max_iter=2000,\n",
       "                                                               random_state=42))]))],\n",
       "                   final_estimator=LogisticRegression(max_iter=1000))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_submission(sc, X, train_tree['target'], X_test, 'stack_meta_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did not improve public score.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking an average wordvecs model, a bow model and a metafeatures model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tbow = ColumnTransformer(transformers=[('bow', CountVectorizer(), 'cleaned_text')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bow = Pipeline(steps=[('sel',tbow),\n",
    "                           ('clf',clf)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "twordvec = ColumnTransformer(transformers=[('wv', UnpackWordvecTransformer(), ['wordvec'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svm = SVC(kernel=\"rbf\",C=1)\n",
    "pipe_wordvec_svm = Pipeline(steps=[('sel',twordvec),\n",
    "                           ('svm',svm)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "sc = StackingClassifier(estimators=[('meta',pipe_meta), \n",
    "                                    ('bow',pipe_bow), \n",
    "                                    ('wordvec_mlp',pipe_nlp), \n",
    "                                    ('wordvec_svm',pipe_wordvec_svm)], \n",
    "                        final_estimator=lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([466.78894591, 370.93477273, 480.38580751, 344.68219972,\n",
       "        320.62175894]),\n",
       " 'score_time': array([3.93788195, 2.66316319, 4.17244959, 2.60680461, 2.61485982]),\n",
       " 'test_score': array([0.75974026, 0.76253081, 0.76744186, 0.76846473, 0.76324503]),\n",
       " 'train_score': array([0.79293139, 0.79426434, 0.80231118, 0.80808081, 0.79097436])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_validate(sc, train_tree, train_tree['target'], cv=cv, return_train_score=True, scoring='f1', n_jobs=-2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764284539416835"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_tree.drop(columns=['target','id'])\n",
    "X_test = test_tree.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   41.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   43.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   50.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   50.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   50.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   52.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('meta',\n",
       "                                Pipeline(steps=[('sel',\n",
       "                                                 ColumnTransformer(transformers=[('wv',\n",
       "                                                                                  UnpackWordvecTransformer(),\n",
       "                                                                                  ['wordvec'])])),\n",
       "                                                ('rf',\n",
       "                                                 GridSearchCV(cv=5,\n",
       "                                                              estimator=LGBMClassifier(n_jobs=-2,\n",
       "                                                                                       random_state=42),\n",
       "                                                              n_jobs=-2,\n",
       "                                                              param_grid={'max_depth': [1,\n",
       "                                                                                        2,\n",
       "                                                                                        3,\n",
       "                                                                                        4],\n",
       "                                                                          'n_estimators': [5,\n",
       "                                                                                           10,\n",
       "                                                                                           20,\n",
       "                                                                                           40,\n",
       "                                                                                           80]},\n",
       "                                                              scoring='f1',\n",
       "                                                              verbose=1))])),\n",
       "                               ('bow',\n",
       "                                Pipeline(steps=[('sel',\n",
       "                                                 ColumnTransformer(transformers=[('bow',\n",
       "                                                                                  CountVectorizer(),\n",
       "                                                                                  'cleaned_text')])),\n",
       "                                                ('clf',\n",
       "                                                 LogisticRegression(max_iter=1000))])),\n",
       "                               ('nlp',\n",
       "                                Pipeline(steps=[('sel',\n",
       "                                                 ColumnTransformer(transformers=[('wv',\n",
       "                                                                                  UnpackWordvecTransformer(),\n",
       "                                                                                  ['wordvec'])])),\n",
       "                                                ('mlp',\n",
       "                                                 MLPClassifier(hidden_layer_sizes=(2,),\n",
       "                                                               learning_rate_init=5e-05,\n",
       "                                                               max_iter=2000,\n",
       "                                                               random_state=42))]))],\n",
       "                   final_estimator=LogisticRegression(max_iter=1000))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_submission(sc, X, train_tree['target'], X_test, 'stack_meta_avgvec_bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
