{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average wordvecs for tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...  \n",
       "1       1             Forest fire near La Ronge Sask. Canada  \n",
       "2       1  All residents asked to 'shelter in place' are ...  \n",
       "3       1  people receive wildfires evacuation orders in ...  \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_geocodes = pd.read_csv('train_geocodes.csv')\n",
    "test_geocodes = pd.read_csv('test_geocodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(train_geocodes, on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(test_geocodes, on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>has_location</th>\n",
       "      <th>geocoded</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  has_location  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...         False   \n",
       "1       1             Forest fire near La Ronge Sask. Canada         False   \n",
       "2       1  All residents asked to 'shelter in place' are ...         False   \n",
       "3       1  people receive wildfires evacuation orders in ...         False   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...         False   \n",
       "\n",
       "   geocoded  longitude  latitude  \n",
       "0     False        NaN       NaN  \n",
       "1     False        NaN       NaN  \n",
       "2     False        NaN       NaN  \n",
       "3     False        NaN       NaN  \n",
       "4     False        NaN       NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>has_location</th>\n",
       "      <th>geocoded</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude_n</th>\n",
       "      <th>latitude_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Heard about earthquake is different cities, st...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>Apocalypse lighting. Spokane wildfires</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>Typhoon Soudelor kills in China and Taiwan</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0                0                 Just happened a terrible car crash   \n",
       "1   2                0  Heard about #earthquake is different cities, s...   \n",
       "2   3                0  there is a forest fire at spot pond, geese are...   \n",
       "3   9                0           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11                0      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                        cleaned_text  has_location  geocoded  \\\n",
       "0                 Just happened a terrible car crash         False     False   \n",
       "1  Heard about earthquake is different cities, st...         False     False   \n",
       "2  there is a forest fire at spot pond, geese are...         False     False   \n",
       "3             Apocalypse lighting. Spokane wildfires         False     False   \n",
       "4         Typhoon Soudelor kills in China and Taiwan         False     False   \n",
       "\n",
       "   longitude  latitude  longitude_n  latitude_n  \n",
       "0        0.0       0.0          0.0         0.0  \n",
       "1        0.0       0.0          0.0         0.0  \n",
       "2        0.0       0.0          0.0         0.0  \n",
       "3        0.0       0.0          0.0         0.0  \n",
       "4        0.0       0.0          0.0         0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['keyword'].fillna('',inplace=True)\n",
    "test['keyword'].fillna('',inplace=True)\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "test['longitude_n'] = test['longitude']/180\n",
    "test['latitude_n'] = test['latitude']/180\n",
    "train['longitude_n'] = train['longitude']/180\n",
    "train['latitude_n'] = train['latitude']/180\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor.defines import Patterns\n",
    "def fe_pattern(df, pattern, name):\n",
    "    df[name] = df['text'].str.lower().apply(lambda x: pattern.findall(x))\n",
    "    df['num_' + name] = df[name].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    fe_pattern(df, Patterns.HASHTAG_PATTERN,'hash')\n",
    "    fe_pattern(df, Patterns.MENTION_PATTERN,'mention')\n",
    "    fe_pattern(df, Patterns.URL_PATTERN,'url')\n",
    "    df['num_hash_n'] = df['num_hash']/train['num_hash'].max()\n",
    "    df['num_mention_n'] = df['num_mention']/train['num_mention'].max()\n",
    "    df['num_url_n'] = df['num_url']/train['num_url'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>has_location</th>\n",
       "      <th>geocoded</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude_n</th>\n",
       "      <th>hash</th>\n",
       "      <th>num_hash</th>\n",
       "      <th>mention</th>\n",
       "      <th>num_mention</th>\n",
       "      <th>url</th>\n",
       "      <th>num_url</th>\n",
       "      <th>num_hash_n</th>\n",
       "      <th>num_mention_n</th>\n",
       "      <th>num_url_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[#earthquake]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[#wildfires]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[#alaska, #wildfires]</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1                0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4                0             Forest fire near La Ronge Sask. Canada   \n",
       "2   5                0  All residents asked to 'shelter in place' are ...   \n",
       "3   6                0  13,000 people receive #wildfires evacuation or...   \n",
       "4   7                0  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  has_location  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...         False   \n",
       "1       1             Forest fire near La Ronge Sask. Canada         False   \n",
       "2       1  All residents asked to 'shelter in place' are ...         False   \n",
       "3       1  people receive wildfires evacuation orders in ...         False   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...         False   \n",
       "\n",
       "   geocoded  longitude  latitude  ...  latitude_n                   hash  \\\n",
       "0     False        0.0       0.0  ...         0.0          [#earthquake]   \n",
       "1     False        0.0       0.0  ...         0.0                     []   \n",
       "2     False        0.0       0.0  ...         0.0                     []   \n",
       "3     False        0.0       0.0  ...         0.0           [#wildfires]   \n",
       "4     False        0.0       0.0  ...         0.0  [#alaska, #wildfires]   \n",
       "\n",
       "  num_hash  mention num_mention  url num_url  num_hash_n  num_mention_n  \\\n",
       "0        1       []           0   []       0    0.076923            0.0   \n",
       "1        0       []           0   []       0    0.000000            0.0   \n",
       "2        0       []           0   []       0    0.000000            0.0   \n",
       "3        1       []           0   []       0    0.076923            0.0   \n",
       "4        2       []           0   []       0    0.153846            0.0   \n",
       "\n",
       "   num_url_n  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "train['cleaned_text'] = train['cleaned_text'].str.translate(table).str.strip()\n",
    "test['cleaned_text'] = test['cleaned_text'].str.translate(table).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(280):\n",
    "    train['cleaned_text'] = train['cleaned_text'].str.replace('  ', ' ')\n",
    "    test['cleaned_text'] = test['cleaned_text'].str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert keywords into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_text_keyword'] = (train['keyword'] + ' ' + train['cleaned_text']).str.strip()\n",
    "test['cleaned_text_keyword'] = (test['keyword'] + ' ' + test['cleaned_text']).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nlp'] = train['cleaned_text'].apply(lambda s: nlp(s))\n",
    "train['wordvec'] = train['nlp'].apply(lambda s: s.vector)\n",
    "test['nlp'] = test['cleaned_text'].apply(lambda s: nlp(s))\n",
    "test['wordvec'] = test['nlp'].apply(lambda s: s.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword_nlp'] = train['keyword'].apply(lambda s:nlp(s))\n",
    "train['keyword_wordvec'] = train['keyword_nlp'].apply(lambda s: s.vector)\n",
    "test['keyword_nlp'] = test['keyword'].apply(lambda s:nlp(s))\n",
    "test['keyword_wordvec'] = test['keyword_nlp'].apply(lambda s: s.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(df, i):\n",
    "    print(train['text'].iloc[i])\n",
    "    print(train['cleaned_text'].iloc[i])\n",
    "    for token in train['nlp'].iloc[i]:\n",
    "        print(token, token.has_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest fire near La Ronge Sask. Canada\n",
      "Forest fire near La Ronge Sask Canada\n",
      "Forest True\n",
      "fire True\n",
      "near True\n",
      "La True\n",
      "Ronge False\n",
      "Sask True\n",
      "Canada True\n"
     ]
    }
   ],
   "source": [
    "check(train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion https://t.co/gFJfgTodad\n",
      "Bahrain police had previously died in a road accident they were not killed by explosion\n",
      "Bahrain True\n",
      "police True\n",
      "had True\n",
      "previously True\n",
      "died True\n",
      "in True\n",
      "a True\n",
      "road True\n",
      "accident True\n",
      "they True\n",
      "were True\n",
      "not True\n",
      "killed True\n",
      "by True\n",
      "explosion True\n"
     ]
    }
   ],
   "source": [
    "check(train, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TradCatKnight (1) Russia may have played into reason but that link is BS.  Okanowa was bloody and mainline invasion looked like a bloody\n",
      "1 Russia may have played into reason but that link is BS Okanowa was bloody and mainline invasion looked like a bloody\n",
      "1 True\n",
      "Russia True\n",
      "may True\n",
      "have True\n",
      "played True\n",
      "into True\n",
      "reason True\n",
      "but True\n",
      "that True\n",
      "link True\n",
      "is True\n",
      "BS True\n",
      "Okanowa False\n",
      "was True\n",
      "bloody True\n",
      "and True\n",
      "mainline True\n",
      "invasion True\n",
      "looked True\n",
      "like True\n",
      "a True\n",
      "bloody True\n"
     ]
    }
   ],
   "source": [
    "check(train, 888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tokens = dict()\n",
    "\n",
    "def gather_tokens(oov_tokens, doc):\n",
    "    for token in doc:\n",
    "        if token.is_oov:\n",
    "            if str(token).lower() in oov_tokens:\n",
    "                oov_tokens[str(token).lower()] += 1\n",
    "            else:\n",
    "                oov_tokens[str(token).lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "3258    None\n",
       "3259    None\n",
       "3260    None\n",
       "3261    None\n",
       "3262    None\n",
       "Name: nlp, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))\n",
    "test['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_df = pd.DataFrame({'token':list(oov_tokens.keys()), 'number':list(oov_tokens.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>mh370</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>prebreak</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>typhoondevastated</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>soudelor</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>funtenna</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>aveblack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>jaxmk2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>fatalityuudlk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>us70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>cityofcalgary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  token  number\n",
       "743               mh370      94\n",
       "1933           prebreak      41\n",
       "1219  typhoondevastated      32\n",
       "817            soudelor      28\n",
       "1932           funtenna      26\n",
       "...                 ...     ...\n",
       "1617           aveblack       1\n",
       "1618             jaxmk2       1\n",
       "1619      fatalityuudlk       1\n",
       "1620               us70       1\n",
       "4348      cityofcalgary       1\n",
       "\n",
       "[4349 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_df = oov_df.sort_values(by='number',ascending=False)\n",
    "oov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafire'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_df.loc[2].token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afflecki'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_df.loc[155].token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mh370' 94]\n",
      " ['prebreak' 41]\n",
      " ['typhoondevastated' 32]\n",
      " ['soudelor' 28]\n",
      " ['funtenna' 26]\n",
      " ['disea' 25]\n",
      " ['gbbo' 23]\n",
      " ['udhampur' 21]\n",
      " ['bayelsa' 21]\n",
      " ['marians' 20]\n",
      " ['enugu' 19]\n",
      " ['utc20150805' 17]\n",
      " ['sensorsenso' 17]\n",
      " ['gtgt' 16]\n",
      " ['selfimage' 16]\n",
      " ['spos' 15]\n",
      " ['time20150806' 14]\n",
      " ['mtvhottest' 13]\n",
      " ['abstorm' 13]\n",
      " ['sismo' 13]\n",
      " ['bestnaijamade' 12]\n",
      " ['mediterran' 12]\n",
      " ['hwo' 11]\n",
      " ['irandeal' 11]\n",
      " ['linkury' 11]\n",
      " ['trfc' 11]\n",
      " ['okwx' 10]\n",
      " ['beyhive' 10]\n",
      " ['o784' 10]\n",
      " ['meatloving' 10]\n",
      " ['yazidis' 10]\n",
      " ['wheavenly' 10]\n",
      " ['sinjar' 10]\n",
      " ['yearold' 10]\n",
      " ['summerfate' 10]\n",
      " ['inj' 9]\n",
      " ['tubestrike' 9]\n",
      " ['chicagoarea' 9]\n",
      " ['breakingnews' 9]\n",
      " ['runion' 9]\n",
      " ['animalrescue' 8]\n",
      " ['trapmusic' 8]\n",
      " ['icemoon' 8]\n",
      " ['igers' 8]\n",
      " ['olap' 8]\n",
      " ['mansehra' 7]\n",
      " ['twia' 7]\n",
      " ['waterresistant' 7]\n",
      " ['explosionproof' 7]\n",
      " ['30pm' 7]\n",
      " ['pantherattack' 7]\n",
      " ['bb17' 7]\n",
      " ['zouma' 7]\n",
      " ['wisenews' 7]\n",
      " ['kisii' 6]\n",
      " ['strategicpatience' 6]\n",
      " ['auspol' 6]\n",
      " ['yycstorm' 6]\n",
      " ['abbswinston' 6]\n",
      " ['gtgtgt' 6]\n",
      " ['kerricktrial' 6]\n",
      " ['lglorg' 6]\n",
      " ['blksamp8whts' 6]\n",
      " ['hostageamp2' 6]\n",
      " ['wbioterrorismampuse' 6]\n",
      " ['saddlebrooke' 6]\n",
      " ['greeces' 6]\n",
      " ['rohingya' 6]\n",
      " ['idis' 6]\n",
      " ['playthursdays' 6]\n",
      " ['bigamist' 6]\n",
      " ['fergusons' 6]\n",
      " ['ramag' 6]\n",
      " ['lulgzimbestpicts' 6]\n",
      " ['listenbuy' 5]\n",
      " ['godslove' 5]\n",
      " ['sittwe' 5]\n",
      " ['gopdebate' 5]\n",
      " ['fettilootch' 5]\n",
      " ['detectado' 5]\n",
      " ['collisionno' 5]\n",
      " ['waimate' 5]\n",
      " ['blowmandyup' 5]\n",
      " ['52015' 5]\n",
      " ['yobe' 5]\n",
      " ['slanglucci' 5]\n",
      " ['beckarnley' 5]\n",
      " ['rockyfire' 5]\n",
      " ['otrametlife' 5]\n",
      " ['pdx911' 5]\n",
      " ['idfire' 5]\n",
      " ['naved' 5]\n",
      " ['metrofmtalk' 5]\n",
      " ['socialnews' 5]\n",
      " ['japn' 5]\n",
      " ['injuryi495' 5]\n",
      " ['i77' 5]\n",
      " ['cecilthelion' 5]\n",
      " ['earthquake' 5]\n",
      " ['bioterrorism' 5]\n",
      " ['thanku' 5]\n",
      " ['weallheartonedirection' 4]\n",
      " ['shelli' 4]\n",
      " ['climatechange' 4]\n",
      " ['time20150805' 4]\n",
      " ['sosfam' 4]\n",
      " ['wedaug5th' 4]\n",
      " ['wedn' 4]\n",
      " ['tripledigit' 4]\n",
      " ['connectorconnecto' 4]\n",
      " ['bb4sp' 4]\n",
      " ['karymsky' 4]\n",
      " ['beforeitsnews' 4]\n",
      " ['tasmanias' 4]\n",
      " ['rapidcity' 4]\n",
      " ['pbban' 4]\n",
      " ['temporary300' 4]\n",
      " ['ultimalucha' 4]\n",
      " ['armageddon' 4]\n",
      " ['humaza' 4]\n",
      " ['ccot' 4]\n",
      " ['sa15' 4]\n",
      " ['demonstratio' 4]\n",
      " ['fennovoima' 4]\n",
      " ['soudelors' 4]\n",
      " ['worstsummerjob' 4]\n",
      " ['isil' 4]\n",
      " ['standwithpp' 4]\n",
      " ['29072015' 4]\n",
      " ['silvergray' 4]\n",
      " ['votejkt48id' 4]\n",
      " ['gunsense' 4]\n",
      " ['hannemans' 4]\n",
      " ['talkradio' 4]\n",
      " ['daesh' 4]\n",
      " ['idps' 4]\n",
      " ['southdowns' 4]\n",
      " ['nikeplus' 4]\n",
      " ['modiministry' 4]\n",
      " ['sigalert' 4]\n",
      " ['m194' 4]\n",
      " ['roh3' 4]\n",
      " ['blacklivesmatter' 4]\n",
      " ['nineyearold' 4]\n",
      " ['massmurderer' 4]\n",
      " ['reaad' 4]\n",
      " ['prophetmuhammad' 4]\n",
      " ['hatcap' 4]\n",
      " ['hiroshima70' 3]\n",
      " ['dorrets' 3]\n",
      " ['globalwarming' 3]\n",
      " ['indiannews' 3]\n",
      " ['plannedparenthood' 3]\n",
      " ['dorret' 3]\n",
      " ['cbcca' 3]\n",
      " ['harda' 3]\n",
      " ['tornadogiveaway' 3]\n",
      " ['15pm' 3]\n",
      " ['kotaweather' 3]\n",
      " ['greatbritishbakeoff' 3]\n",
      " ['topstories' 3]\n",
      " ['fieg' 3]\n",
      " ['rworldnews' 3]\n",
      " ['collisionunkn' 3]\n",
      " ['luchaunderground' 3]\n",
      " ['tcot' 3]\n",
      " ['bioterror' 3]\n",
      " ['rickperry' 3]\n",
      " ['slosher' 3]\n",
      " ['bookboost' 3]\n",
      " ['faroeislands' 3]\n",
      " ['debatequestionswewanttohear' 3]\n",
      " ['ibooklove' 3]\n",
      " ['aoms' 3]\n",
      " ['icymi' 3]\n",
      " ['gtii' 3]\n",
      " ['bakeofffriends' 3]\n",
      " ['i405' 3]\n",
      " ['askconnor' 3]\n",
      " ['treeporn' 3]\n",
      " ['njturnpike' 3]\n",
      " ['multidimensi' 3]\n",
      " ['throwingknifes' 3]\n",
      " ['humanconsumption' 3]\n",
      " ['magginoodle' 3]\n",
      " ['nestleindia' 3]\n",
      " ['offers2go' 3]\n",
      " ['reactorbased' 3]\n",
      " ['indojapan' 3]\n",
      " ['nucleardeal' 3]\n",
      " ['foodscare' 3]\n",
      " ['artistsunited' 3]\n",
      " ['linerless' 3]\n",
      " ['ekdar' 3]\n",
      " ['collision1141' 3]\n",
      " ['enrt' 3]\n",
      " ['politifiact' 3]\n",
      " ['sixmeter' 3]\n",
      " ['harrybecareful' 3]\n",
      " ['magnetraction' 3]\n",
      " ['hempoil' 3]\n",
      " ['standuser' 3]\n",
      " ['haiyan' 3]\n",
      " ['gilbert23' 3]\n",
      " ['commoditiesare' 3]\n",
      " ['calgarys' 3]\n",
      " ['latestnews' 3]\n",
      " ['followme' 3]\n",
      " ['exp0sed' 3]\n",
      " ['festac' 3]\n",
      " ['s3xleakph0tos' 3]\n",
      " ['bacup' 3]\n",
      " ['literarycakes' 3]\n",
      " ['bluedio' 3]\n",
      " ['hannaph' 3]\n",
      " ['soudelortropical' 3]\n",
      " ['propertycasualty' 3]\n",
      " ['grupdates' 3]\n",
      " ['abuseddesolateamplost' 3]\n",
      " ['whelen' 3]\n",
      " ['letsfootball' 3]\n",
      " ['hermancranston' 3]\n",
      " ['undergroundrailraod' 3]\n",
      " ['offensivecontent' 3]\n",
      " ['arwx' 3]\n",
      " ['yugvani' 3]\n",
      " ['onlinecommunities' 3]\n",
      " ['tilnow' 3]\n",
      " ['noonanheartbreak' 3]\n",
      " ['yahistorical' 3]\n",
      " ['sportwatch' 3]\n",
      " ['ss100' 3]\n",
      " ['bhramabull' 3]\n",
      " ['gamergate' 3]\n",
      " ['uptotheminute' 3]\n",
      " ['clearedincident' 3]\n",
      " ['apch' 3]\n",
      " ['oper' 3]\n",
      " ['navbl' 3]\n",
      " ['warningissued' 3]\n",
      " ['efak' 3]\n",
      " ['liveonk2' 3]\n",
      " ['mido' 3]\n",
      " ['putins' 3]\n",
      " ['earththus' 3]\n",
      " ['bankstown' 3]\n",
      " ['versethe' 3]\n",
      " ['mhtw4fnetofficials' 3]\n",
      " ['whatsmentioned' 3]\n",
      " ['thebeginning' 3]\n",
      " ['littledeath' 3]\n",
      " ['ruebs' 3]\n",
      " ['rightways' 3]\n",
      " ['harmkid' 3]\n",
      " ['videoveranomtv' 3]\n",
      " ['photoset' 3]\n",
      " ['mmda' 3]\n",
      " ['tgf2015' 3]\n",
      " ['savebees' 3]\n",
      " ['newsintweets' 3]\n",
      " ['listenlive' 3]\n",
      " ['usnwsgov' 3]\n",
      " ['salopek' 3]\n",
      " ['abomb' 3]\n",
      " ['diesis' 3]\n",
      " ['banki' 3]\n",
      " ['pjnet' 3]\n",
      " ['koin6news' 3]\n",
      " ['buddys' 3]\n",
      " ['calgaryweather' 2]\n",
      " ['newsdict' 2]\n",
      " ['i580' 2]\n",
      " ['stagefright' 2]\n",
      " ['jamaicaplain' 2]\n",
      " ['ofclans' 2]\n",
      " ['buffetts' 2]\n",
      " ['offr' 2]\n",
      " ['eightynine' 2]\n",
      " ['ptsdchat' 2]\n",
      " ['sr37' 2]\n",
      " ['mi17' 2]\n",
      " ['twovehicle' 2]\n",
      " ['yycweather' 2]\n",
      " ['mhtw4fnetpakistan' 2]\n",
      " ['traumatised' 2]\n",
      " ['pcps' 2]\n",
      " ['muzzamil' 2]\n",
      " ['38pm' 2]\n",
      " ['techesback' 2]\n",
      " ['skanndtyagi' 2]\n",
      " ['carryi' 2]\n",
      " ['yazidi' 2]\n",
      " ['janaq' 2]\n",
      " ['meteoearth' 2]\n",
      " ['angelriveralib' 2]\n",
      " ['buildingswe' 2]\n",
      " ['dhsscitech' 2]\n",
      " ['forestservice' 2]\n",
      " ['lightningcaused' 2]\n",
      " ['fatburning' 2]\n",
      " ['renew911health' 2]\n",
      " ['threealarm' 2]\n",
      " ['pulwama' 2]\n",
      " ['wmur9' 2]\n",
      " ['autoinsurance' 2]\n",
      " ['22pm' 2]\n",
      " ['tweetlikeitsseptember11th2001' 2]\n",
      " ['cbplawyers' 2]\n",
      " ['homerescuers' 2]\n",
      " ['strikesstrikes' 2]\n",
      " ['newsmornings' 2]\n",
      " ['oworoshoki' 2]\n",
      " ['bstop' 2]\n",
      " ['urgentthere' 2]\n",
      " ['antiterrorism' 2]\n",
      " ['tlvfaces' 2]\n",
      " ['zarry' 2]\n",
      " ['internallydisplaced' 2]\n",
      " ['c4news' 2]\n",
      " ['eudrylantiqua' 2]\n",
      " ['capsizes' 2]\n",
      " ['walerga' 2]\n",
      " ['lasvegas' 2]\n",
      " ['wdsu' 2]\n",
      " ['ptbo' 2]\n",
      " ['newswatch' 2]\n",
      " ['ebike' 2]\n",
      " ['thebookclub' 2]\n",
      " ['oklahomaok' 2]\n",
      " ['40pm' 2]\n",
      " ['pletchs' 2]\n",
      " ['roh3smantibatam' 2]\n",
      " ['smantibatam' 2]\n",
      " ['alrasyid448iturasya' 2]\n",
      " ['cadfyi' 2]\n",
      " ['mcgsecure' 2]\n",
      " ['liveleakfun' 2]\n",
      " ['notexplained' 2]\n",
      " ['softenza' 2]\n",
      " ['piner' 2]\n",
      " ['rdhorndale' 2]\n",
      " ['nuclearbiologicalchemical' 2]\n",
      " ['iredell' 2]\n",
      " ['072015' 2]\n",
      " ['fahlo' 2]\n",
      " ['sr14' 2]\n",
      " ['foxa' 2]\n",
      " ['cmcsa' 2]\n",
      " ['amcx' 2]\n",
      " ['viab' 2]\n",
      " ['twx' 2]\n",
      " ['ifak' 2]\n",
      " ['zabadani' 2]\n",
      " ['schwarber' 2]\n",
      " ['ahrar' 2]\n",
      " ['thisiswhywecanthavenicethings' 2]\n",
      " ['glink' 2]\n",
      " ['olhead' 2]\n",
      " ['marquei' 2]\n",
      " ['bancodeseries' 2]\n",
      " ['feelingmanly' 2]\n",
      " ['yiayplan' 2]\n",
      " ['japton' 2]\n",
      " ['wpri' 2]\n",
      " ['ttes' 2]\n",
      " ['quivk' 2]\n",
      " ['demolitiondodging' 2]\n",
      " ['enemity' 2]\n",
      " ['wftv' 2]\n",
      " ['redeemeth' 2]\n",
      " ['comingsoon' 2]\n",
      " ['shantae' 2]\n",
      " ['defundpp' 2]\n",
      " ['fotoset' 2]\n",
      " ['timelapse' 2]\n",
      " ['alcoholismaddiction' 2]\n",
      " ['gaelite' 2]\n",
      " ['gmmbc' 2]\n",
      " ['fifa16' 2]\n",
      " ['lv6' 2]\n",
      " ['motorcraft' 2]\n",
      " ['gawx' 2]\n",
      " ['ihhen' 2]\n",
      " ['nankana' 2]\n",
      " ['okthe' 2]\n",
      " ['considerelem' 2]\n",
      " ['trueheroes' 2]\n",
      " ['scwx' 2]\n",
      " ['pakpattan' 2]\n",
      " ['ks94' 2]\n",
      " ['wouldelectrocute' 2]\n",
      " ['emsc' 2]\n",
      " ['twotone' 2]\n",
      " ['crossbody' 2]\n",
      " ['renison' 2]\n",
      " ['cafire' 2]\n",
      " ['bago' 2]\n",
      " ['archipelagowolves' 2]\n",
      " ['kimery' 2]\n",
      " ['protectdenaliwolves' 2]\n",
      " ['msica' 2]\n",
      " ['jakartapost' 2]\n",
      " ['ti5' 2]\n",
      " ['wroug' 2]\n",
      " ['missionhills' 2]\n",
      " ['ivanberroa' 2]\n",
      " ['losdelsonido' 2]\n",
      " ['hammondville' 2]\n",
      " ['perrie' 2]\n",
      " ['hitchbot' 2]\n",
      " ['inec' 2]\n",
      " ['sensorknock' 2]\n",
      " ['sr91' 2]\n",
      " ['stagetwo' 2]\n",
      " ['jonathanferrell' 2]\n",
      " ['chesttorso' 2]\n",
      " ['albertans' 2]\n",
      " ['ef5' 2]\n",
      " ['wxky' 2]\n",
      " ['alwx' 2]\n",
      " ['askcharley' 2]\n",
      " ['raung' 2]\n",
      " ['monthold' 2]\n",
      " ['aogashima' 2]\n",
      " ['publichealth' 2]\n",
      " ['diaporama' 2]\n",
      " ['sixpenceee' 2]\n",
      " ['volcanoinrussia' 2]\n",
      " ['utc5' 2]\n",
      " ['insas' 2]\n",
      " ['us101' 2]\n",
      " ['stormchase' 2]\n",
      " ['cawx' 2]\n",
      " ['diageos' 2]\n",
      " ['thegame' 2]\n",
      " ['valleywx' 2]\n",
      " ['nycha' 2]\n",
      " ['memorialday' 2]\n",
      " ['pagasa' 2]\n",
      " ['lowlying' 2]\n",
      " ['worldvision' 2]\n",
      " ['jhaustin' 2]\n",
      " ['routecomplex' 2]\n",
      " ['outbid' 2]\n",
      " ['freefrom' 2]\n",
      " ['shania' 2]\n",
      " ['riversiskiyou' 2]\n",
      " ['dothraki' 2]\n",
      " ['seasonfrom' 2]\n",
      " ['onshit' 2]\n",
      " ['ushed' 2]\n",
      " ['whitbourne' 2]\n",
      " ['sn' 2]\n",
      " ['salvis' 2]\n",
      " ['windstorm' 2]\n",
      " ['achimota' 2]\n",
      " ['canberras' 2]\n",
      " ['tookem' 2]\n",
      " ['alllivesmatter' 2]\n",
      " ['kowing' 2]\n",
      " ['zippednews' 2]\n",
      " ['news3lv' 2]\n",
      " ['upwindstorm' 2]\n",
      " ['sixcar' 2]\n",
      " ['identitytheft' 2]\n",
      " ['justmarried' 2]\n",
      " ['vabengal' 2]\n",
      " ['irin' 2]\n",
      " ['easternoregon' 2]\n",
      " ['2676773' 2]\n",
      " ['mhtw4fnetcrews' 2]\n",
      " ['fingerrockfire' 2]\n",
      " ['njenga' 2]\n",
      " ['growingupblack' 2]\n",
      " ['birminghams' 2]\n",
      " ['unitedstates' 2]\n",
      " ['pawsox' 2]\n",
      " ['emergencyresponse' 2]\n",
      " ['lukebox' 2]\n",
      " ['infoorder' 2]\n",
      " ['sms087809233445' 2]\n",
      " ['insanelimits' 2]\n",
      " ['beclearoncancer' 2]\n",
      " ['engvaus' 2]\n",
      " ['lavenderpoetrycafe' 2]\n",
      " ['daytoday' 2]\n",
      " ['yakub' 2]\n",
      " ['rpics' 2]\n",
      " ['nasahurricane' 2]\n",
      " ['02pm' 2]\n",
      " ['saumur' 2]\n",
      " ['icaseit' 2]\n",
      " ['vibez' 2]\n",
      " ['dealsuk' 2]\n",
      " ['win10' 2]\n",
      " ['magner' 2]\n",
      " ['m151a1' 2]\n",
      " ['saltriverwildhorses' 2]\n",
      " ['ovofest' 2]\n",
      " ['bodybagging' 2]]\n"
     ]
    }
   ],
   "source": [
    "print(oov_df.head(500).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of tokens oov:  0.04076309050291505\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of tokens oov: \", \n",
    "      oov_df.number.sum() / (train['nlp'].apply(len).sum() + test['nlp'].apply(len).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To explore: use wordninja to cut up composite words/hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['typhoon', 'devastated']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('typhoondevastated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mh', '370']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('mh370')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre', 'break']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('prebreak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def get_X(df):\n",
    "    X = []\n",
    "    X_ext = []\n",
    "    xcols = ['has_location', 'geocoded','longitude_n','latitude_n','num_hash_n','num_mention_n','num_url_n']\n",
    "    for index, row in df.iterrows():\n",
    "        x = row['wordvec']\n",
    "        #x = numpy.append(x, row['keyword_wordvec'])\n",
    "        \n",
    "        X.append(x)\n",
    "        for xc in xcols:\n",
    "            x = numpy.append(x, row[xc])        \n",
    "        X_ext.append(x)\n",
    "    return X, X_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_ext = get_X(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svm = SVC(kernel=\"rbf\")\n",
    "params = {'C': [0.05, 0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 4]}\n",
    "clf = GridSearchCV(svm, params, scoring=\"f1\", verbose=1, n_jobs=-2, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([102.75017309, 111.90411711, 129.97967076]),\n",
       " 'score_time': array([3.09891152, 2.84578514, 3.09524584]),\n",
       " 'test_score': array([0.73401397, 0.7159035 , 0.78023033]),\n",
       " 'train_score': array([0.79595449, 0.81373044, 0.80443548])}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(clf, X, y, cv=3, return_train_score=True, scoring='f1')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7433825976979325"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(model, X, y, X_test, name):\n",
    "    fit = model.fit(X,y)\n",
    "    pred = model.predict(X_test)\n",
    "    submission = pd.DataFrame({\"id\":test['id'], \"target\":pred})\n",
    "    submission.to_csv(name+'.csv', index=False)\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  4.4min finished\n"
     ]
    }
   ],
   "source": [
    "X_test, X_test_ext = get_X(test)\n",
    "fit = prepare_submission(clf, X, y, X_test, 'avg_wordvec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([16.2656002 , 16.06399727, 14.81328235, 13.62246323, 13.98820539,\n",
       "        13.89963684, 13.16968436, 12.98290372, 13.06896777]),\n",
       " 'std_fit_time': array([1.16269804, 0.50647231, 0.76079187, 1.23859031, 0.82218409,\n",
       "        0.2663644 , 0.57257947, 0.71957938, 1.06073153]),\n",
       " 'mean_score_time': array([3.66964693, 3.62519808, 3.01858544, 2.94939013, 2.89965215,\n",
       "        2.81909885, 2.7464879 , 2.89028668, 2.63742886]),\n",
       " 'std_score_time': array([0.52700126, 0.33103732, 0.29818703, 0.29587163, 0.23702686,\n",
       "        0.22736502, 0.31430439, 0.32260753, 0.297495  ]),\n",
       " 'param_C': masked_array(data=[0.05, 0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.05},\n",
       "  {'C': 0.1},\n",
       "  {'C': 0.2},\n",
       "  {'C': 0.5},\n",
       "  {'C': 0.75},\n",
       "  {'C': 1},\n",
       "  {'C': 1.5},\n",
       "  {'C': 2},\n",
       "  {'C': 4}],\n",
       " 'split0_test_score': array([0.71715328, 0.72760181, 0.7277677 , 0.72661871, 0.7264574 ,\n",
       "        0.72987478, 0.72922252, 0.730187  , 0.72759539]),\n",
       " 'split1_test_score': array([0.69625762, 0.70232959, 0.71012007, 0.71864407, 0.72512648,\n",
       "        0.72527473, 0.71959459, 0.71212121, 0.70315091]),\n",
       " 'split2_test_score': array([0.71896552, 0.7306712 , 0.73781513, 0.7442623 , 0.74104235,\n",
       "        0.74350649, 0.74097835, 0.74221868, 0.73659306]),\n",
       " 'split3_test_score': array([0.6972639 , 0.70680628, 0.71242398, 0.70199826, 0.70408163,\n",
       "        0.69811321, 0.69699571, 0.69819433, 0.69439728]),\n",
       " 'split4_test_score': array([0.76767677, 0.76705491, 0.77435065, 0.77804681, 0.78162772,\n",
       "        0.78205128, 0.78691141, 0.78281623, 0.77647059]),\n",
       " 'mean_test_score': array([0.71946342, 0.72689276, 0.7324955 , 0.73391403, 0.73566711,\n",
       "        0.7357641 , 0.73474052, 0.73310749, 0.72764145]),\n",
       " 'std_test_score': array([0.02592852, 0.02295471, 0.02326383, 0.02592219, 0.02582571,\n",
       "        0.02743993, 0.02981688, 0.02905806, 0.02887691]),\n",
       " 'rank_test_score': array([9, 8, 6, 4, 2, 1, 3, 5, 7], dtype=int32)}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
