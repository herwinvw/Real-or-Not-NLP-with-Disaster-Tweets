{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average wordvecs for tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...  \n",
       "1       1             Forest fire near La Ronge Sask. Canada  \n",
       "2       1  All residents asked to 'shelter in place' are ...  \n",
       "3       1  people receive wildfires evacuation orders in ...  \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "train['cleaned_text'] = train['cleaned_text'].str.translate(table).str.strip()\n",
    "test['cleaned_text'] = test['cleaned_text'].str.translate(table).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(280):\n",
    "    train['cleaned_text'] = train['cleaned_text'].str.replace('  ', ' ')\n",
    "    test['cleaned_text'] = test['cleaned_text'].str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert keywords into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].fillna('',inplace=True)\n",
    "test['keyword'].fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_text_keyword'] = (train['keyword'] + ' ' + train['cleaned_text']).str.strip()\n",
    "test['cleaned_text_keyword'] = (test['keyword'] + ' ' + test['cleaned_text']).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nlp'] = train['cleaned_text'].apply(lambda s: nlp(s))\n",
    "train['wordvec'] = train['nlp'].apply(lambda s: s.vector)\n",
    "test['nlp'] = test['cleaned_text'].apply(lambda s: nlp(s))\n",
    "test['wordvec'] = test['nlp'].apply(lambda s: s.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword_nlp'] = train['keyword'].apply(lambda s:nlp(s))\n",
    "train['keyword_wordvec'] = train['keyword_nlp'].apply(lambda s: s.vector)\n",
    "test['keyword_nlp'] = test['keyword'].apply(lambda s:nlp(s))\n",
    "test['keyword_wordvec'] = test['keyword_nlp'].apply(lambda s: s.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(df, i):\n",
    "    print(train['text'].iloc[i])\n",
    "    print(train['cleaned_text'].iloc[i])\n",
    "    for token in train['nlp'].iloc[i]:\n",
    "        print(token, token.has_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest fire near La Ronge Sask. Canada\n",
      "Forest fire near La Ronge Sask Canada\n",
      "Forest True\n",
      "fire True\n",
      "near True\n",
      "La True\n",
      "Ronge False\n",
      "Sask True\n",
      "Canada True\n"
     ]
    }
   ],
   "source": [
    "check(train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion https://t.co/gFJfgTodad\n",
      "Bahrain police had previously died in a road accident they were not killed by explosion\n",
      "Bahrain True\n",
      "police True\n",
      "had True\n",
      "previously True\n",
      "died True\n",
      "in True\n",
      "a True\n",
      "road True\n",
      "accident True\n",
      "they True\n",
      "were True\n",
      "not True\n",
      "killed True\n",
      "by True\n",
      "explosion True\n"
     ]
    }
   ],
   "source": [
    "check(train, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TradCatKnight (1) Russia may have played into reason but that link is BS.  Okanowa was bloody and mainline invasion looked like a bloody\n",
      "1 Russia may have played into reason but that link is BS Okanowa was bloody and mainline invasion looked like a bloody\n",
      "1 True\n",
      "Russia True\n",
      "may True\n",
      "have True\n",
      "played True\n",
      "into True\n",
      "reason True\n",
      "but True\n",
      "that True\n",
      "link True\n",
      "is True\n",
      "BS True\n",
      "Okanowa False\n",
      "was True\n",
      "bloody True\n",
      "and True\n",
      "mainline True\n",
      "invasion True\n",
      "looked True\n",
      "like True\n",
      "a True\n",
      "bloody True\n"
     ]
    }
   ],
   "source": [
    "check(train, 888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tokens = dict()\n",
    "\n",
    "def gather_tokens(oov_tokens, doc):\n",
    "    for token in doc:\n",
    "        if token.is_oov:\n",
    "            if str(token).lower() in oov_tokens:\n",
    "                oov_tokens[str(token).lower()] += 1\n",
    "            else:\n",
    "                oov_tokens[str(token).lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "3258    None\n",
       "3259    None\n",
       "3260    None\n",
       "3261    None\n",
       "3262    None\n",
       "Name: nlp, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))\n",
    "test['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_df = pd.DataFrame({'token':list(oov_tokens.keys()), 'number':list(oov_tokens.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>mh370</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>prebreak</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>typhoondevastated</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>soudelor</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>funtenna</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>aveblack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>jaxmk2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>fatalityuudlk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>us70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>cityofcalgary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  token  number\n",
       "743               mh370      94\n",
       "1933           prebreak      41\n",
       "1219  typhoondevastated      32\n",
       "817            soudelor      28\n",
       "1932           funtenna      26\n",
       "...                 ...     ...\n",
       "1617           aveblack       1\n",
       "1618             jaxmk2       1\n",
       "1619      fatalityuudlk       1\n",
       "1620               us70       1\n",
       "4348      cityofcalgary       1\n",
       "\n",
       "[4349 rows x 2 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_df = oov_df.sort_values(by='number',ascending=False)\n",
    "oov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafire'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_df.loc[2].token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afflecki'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_df.loc[155].token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mh370' 94]\n",
      " ['prebreak' 41]\n",
      " ['typhoondevastated' 32]\n",
      " ['soudelor' 28]\n",
      " ['funtenna' 26]\n",
      " ['disea' 25]\n",
      " ['gbbo' 23]\n",
      " ['udhampur' 21]\n",
      " ['bayelsa' 21]\n",
      " ['marians' 20]\n",
      " ['enugu' 19]\n",
      " ['utc20150805' 17]\n",
      " ['sensorsenso' 17]\n",
      " ['gtgt' 16]\n",
      " ['selfimage' 16]\n",
      " ['spos' 15]\n",
      " ['time20150806' 14]\n",
      " ['mtvhottest' 13]\n",
      " ['abstorm' 13]\n",
      " ['sismo' 13]\n",
      " ['bestnaijamade' 12]\n",
      " ['mediterran' 12]\n",
      " ['hwo' 11]\n",
      " ['irandeal' 11]\n",
      " ['linkury' 11]\n",
      " ['trfc' 11]\n",
      " ['okwx' 10]\n",
      " ['beyhive' 10]\n",
      " ['o784' 10]\n",
      " ['meatloving' 10]\n",
      " ['yazidis' 10]\n",
      " ['wheavenly' 10]\n",
      " ['sinjar' 10]\n",
      " ['yearold' 10]\n",
      " ['summerfate' 10]\n",
      " ['inj' 9]\n",
      " ['tubestrike' 9]\n",
      " ['chicagoarea' 9]\n",
      " ['breakingnews' 9]\n",
      " ['runion' 9]\n",
      " ['animalrescue' 8]\n",
      " ['trapmusic' 8]\n",
      " ['icemoon' 8]\n",
      " ['igers' 8]\n",
      " ['olap' 8]\n",
      " ['mansehra' 7]\n",
      " ['twia' 7]\n",
      " ['waterresistant' 7]\n",
      " ['explosionproof' 7]\n",
      " ['30pm' 7]\n",
      " ['pantherattack' 7]\n",
      " ['bb17' 7]\n",
      " ['zouma' 7]\n",
      " ['wisenews' 7]\n",
      " ['kisii' 6]\n",
      " ['strategicpatience' 6]\n",
      " ['auspol' 6]\n",
      " ['yycstorm' 6]\n",
      " ['abbswinston' 6]\n",
      " ['gtgtgt' 6]\n",
      " ['kerricktrial' 6]\n",
      " ['lglorg' 6]\n",
      " ['blksamp8whts' 6]\n",
      " ['hostageamp2' 6]\n",
      " ['wbioterrorismampuse' 6]\n",
      " ['saddlebrooke' 6]\n",
      " ['greeces' 6]\n",
      " ['rohingya' 6]\n",
      " ['idis' 6]\n",
      " ['playthursdays' 6]\n",
      " ['bigamist' 6]\n",
      " ['fergusons' 6]\n",
      " ['ramag' 6]\n",
      " ['lulgzimbestpicts' 6]\n",
      " ['listenbuy' 5]\n",
      " ['godslove' 5]\n",
      " ['sittwe' 5]\n",
      " ['gopdebate' 5]\n",
      " ['fettilootch' 5]\n",
      " ['detectado' 5]\n",
      " ['collisionno' 5]\n",
      " ['waimate' 5]\n",
      " ['blowmandyup' 5]\n",
      " ['52015' 5]\n",
      " ['yobe' 5]\n",
      " ['slanglucci' 5]\n",
      " ['beckarnley' 5]\n",
      " ['rockyfire' 5]\n",
      " ['otrametlife' 5]\n",
      " ['pdx911' 5]\n",
      " ['idfire' 5]\n",
      " ['naved' 5]\n",
      " ['metrofmtalk' 5]\n",
      " ['socialnews' 5]\n",
      " ['japn' 5]\n",
      " ['injuryi495' 5]\n",
      " ['i77' 5]\n",
      " ['cecilthelion' 5]\n",
      " ['earthquake' 5]\n",
      " ['bioterrorism' 5]\n",
      " ['thanku' 5]\n",
      " ['weallheartonedirection' 4]\n",
      " ['shelli' 4]\n",
      " ['climatechange' 4]\n",
      " ['time20150805' 4]\n",
      " ['sosfam' 4]\n",
      " ['wedaug5th' 4]\n",
      " ['wedn' 4]\n",
      " ['tripledigit' 4]\n",
      " ['connectorconnecto' 4]\n",
      " ['bb4sp' 4]\n",
      " ['karymsky' 4]\n",
      " ['beforeitsnews' 4]\n",
      " ['tasmanias' 4]\n",
      " ['rapidcity' 4]\n",
      " ['pbban' 4]\n",
      " ['temporary300' 4]\n",
      " ['ultimalucha' 4]\n",
      " ['armageddon' 4]\n",
      " ['humaza' 4]\n",
      " ['ccot' 4]\n",
      " ['sa15' 4]\n",
      " ['demonstratio' 4]\n",
      " ['fennovoima' 4]\n",
      " ['soudelors' 4]\n",
      " ['worstsummerjob' 4]\n",
      " ['isil' 4]\n",
      " ['standwithpp' 4]\n",
      " ['29072015' 4]\n",
      " ['silvergray' 4]\n",
      " ['votejkt48id' 4]\n",
      " ['gunsense' 4]\n",
      " ['hannemans' 4]\n",
      " ['talkradio' 4]\n",
      " ['daesh' 4]\n",
      " ['idps' 4]\n",
      " ['southdowns' 4]\n",
      " ['nikeplus' 4]\n",
      " ['modiministry' 4]\n",
      " ['sigalert' 4]\n",
      " ['m194' 4]\n",
      " ['roh3' 4]\n",
      " ['blacklivesmatter' 4]\n",
      " ['nineyearold' 4]\n",
      " ['massmurderer' 4]\n",
      " ['reaad' 4]\n",
      " ['prophetmuhammad' 4]\n",
      " ['hatcap' 4]\n",
      " ['hiroshima70' 3]\n",
      " ['dorrets' 3]\n",
      " ['globalwarming' 3]\n",
      " ['indiannews' 3]\n",
      " ['plannedparenthood' 3]\n",
      " ['dorret' 3]\n",
      " ['cbcca' 3]\n",
      " ['harda' 3]\n",
      " ['tornadogiveaway' 3]\n",
      " ['15pm' 3]\n",
      " ['kotaweather' 3]\n",
      " ['greatbritishbakeoff' 3]\n",
      " ['topstories' 3]\n",
      " ['fieg' 3]\n",
      " ['rworldnews' 3]\n",
      " ['collisionunkn' 3]\n",
      " ['luchaunderground' 3]\n",
      " ['tcot' 3]\n",
      " ['bioterror' 3]\n",
      " ['rickperry' 3]\n",
      " ['slosher' 3]\n",
      " ['bookboost' 3]\n",
      " ['faroeislands' 3]\n",
      " ['debatequestionswewanttohear' 3]\n",
      " ['ibooklove' 3]\n",
      " ['aoms' 3]\n",
      " ['icymi' 3]\n",
      " ['gtii' 3]\n",
      " ['bakeofffriends' 3]\n",
      " ['i405' 3]\n",
      " ['askconnor' 3]\n",
      " ['treeporn' 3]\n",
      " ['njturnpike' 3]\n",
      " ['multidimensi' 3]\n",
      " ['throwingknifes' 3]\n",
      " ['humanconsumption' 3]\n",
      " ['magginoodle' 3]\n",
      " ['nestleindia' 3]\n",
      " ['offers2go' 3]\n",
      " ['reactorbased' 3]\n",
      " ['indojapan' 3]\n",
      " ['nucleardeal' 3]\n",
      " ['foodscare' 3]\n",
      " ['artistsunited' 3]\n",
      " ['linerless' 3]\n",
      " ['ekdar' 3]\n",
      " ['collision1141' 3]\n",
      " ['enrt' 3]\n",
      " ['politifiact' 3]\n",
      " ['sixmeter' 3]\n",
      " ['harrybecareful' 3]\n",
      " ['magnetraction' 3]\n",
      " ['hempoil' 3]\n",
      " ['standuser' 3]\n",
      " ['haiyan' 3]\n",
      " ['gilbert23' 3]\n",
      " ['commoditiesare' 3]\n",
      " ['calgarys' 3]\n",
      " ['latestnews' 3]\n",
      " ['followme' 3]\n",
      " ['exp0sed' 3]\n",
      " ['festac' 3]\n",
      " ['s3xleakph0tos' 3]\n",
      " ['bacup' 3]\n",
      " ['literarycakes' 3]\n",
      " ['bluedio' 3]\n",
      " ['hannaph' 3]\n",
      " ['soudelortropical' 3]\n",
      " ['propertycasualty' 3]\n",
      " ['grupdates' 3]\n",
      " ['abuseddesolateamplost' 3]\n",
      " ['whelen' 3]\n",
      " ['letsfootball' 3]\n",
      " ['hermancranston' 3]\n",
      " ['undergroundrailraod' 3]\n",
      " ['offensivecontent' 3]\n",
      " ['arwx' 3]\n",
      " ['yugvani' 3]\n",
      " ['onlinecommunities' 3]\n",
      " ['tilnow' 3]\n",
      " ['noonanheartbreak' 3]\n",
      " ['yahistorical' 3]\n",
      " ['sportwatch' 3]\n",
      " ['ss100' 3]\n",
      " ['bhramabull' 3]\n",
      " ['gamergate' 3]\n",
      " ['uptotheminute' 3]\n",
      " ['clearedincident' 3]\n",
      " ['apch' 3]\n",
      " ['oper' 3]\n",
      " ['navbl' 3]\n",
      " ['warningissued' 3]\n",
      " ['efak' 3]\n",
      " ['liveonk2' 3]\n",
      " ['mido' 3]\n",
      " ['putins' 3]\n",
      " ['earththus' 3]\n",
      " ['bankstown' 3]\n",
      " ['versethe' 3]\n",
      " ['mhtw4fnetofficials' 3]\n",
      " ['whatsmentioned' 3]\n",
      " ['thebeginning' 3]\n",
      " ['littledeath' 3]\n",
      " ['ruebs' 3]\n",
      " ['rightways' 3]\n",
      " ['harmkid' 3]\n",
      " ['videoveranomtv' 3]\n",
      " ['photoset' 3]\n",
      " ['mmda' 3]\n",
      " ['tgf2015' 3]\n",
      " ['savebees' 3]\n",
      " ['newsintweets' 3]\n",
      " ['listenlive' 3]\n",
      " ['usnwsgov' 3]\n",
      " ['salopek' 3]\n",
      " ['abomb' 3]\n",
      " ['diesis' 3]\n",
      " ['banki' 3]\n",
      " ['pjnet' 3]\n",
      " ['koin6news' 3]\n",
      " ['buddys' 3]\n",
      " ['calgaryweather' 2]\n",
      " ['newsdict' 2]\n",
      " ['i580' 2]\n",
      " ['stagefright' 2]\n",
      " ['jamaicaplain' 2]\n",
      " ['ofclans' 2]\n",
      " ['buffetts' 2]\n",
      " ['offr' 2]\n",
      " ['eightynine' 2]\n",
      " ['ptsdchat' 2]\n",
      " ['sr37' 2]\n",
      " ['mi17' 2]\n",
      " ['twovehicle' 2]\n",
      " ['yycweather' 2]\n",
      " ['mhtw4fnetpakistan' 2]\n",
      " ['traumatised' 2]\n",
      " ['pcps' 2]\n",
      " ['muzzamil' 2]\n",
      " ['38pm' 2]\n",
      " ['techesback' 2]\n",
      " ['skanndtyagi' 2]\n",
      " ['carryi' 2]\n",
      " ['yazidi' 2]\n",
      " ['janaq' 2]\n",
      " ['meteoearth' 2]\n",
      " ['angelriveralib' 2]\n",
      " ['buildingswe' 2]\n",
      " ['dhsscitech' 2]\n",
      " ['forestservice' 2]\n",
      " ['lightningcaused' 2]\n",
      " ['fatburning' 2]\n",
      " ['renew911health' 2]\n",
      " ['threealarm' 2]\n",
      " ['pulwama' 2]\n",
      " ['wmur9' 2]\n",
      " ['autoinsurance' 2]\n",
      " ['22pm' 2]\n",
      " ['tweetlikeitsseptember11th2001' 2]\n",
      " ['cbplawyers' 2]\n",
      " ['homerescuers' 2]\n",
      " ['strikesstrikes' 2]\n",
      " ['newsmornings' 2]\n",
      " ['oworoshoki' 2]\n",
      " ['bstop' 2]\n",
      " ['urgentthere' 2]\n",
      " ['antiterrorism' 2]\n",
      " ['tlvfaces' 2]\n",
      " ['zarry' 2]\n",
      " ['internallydisplaced' 2]\n",
      " ['c4news' 2]\n",
      " ['eudrylantiqua' 2]\n",
      " ['capsizes' 2]\n",
      " ['walerga' 2]\n",
      " ['lasvegas' 2]\n",
      " ['wdsu' 2]\n",
      " ['ptbo' 2]\n",
      " ['newswatch' 2]\n",
      " ['ebike' 2]\n",
      " ['thebookclub' 2]\n",
      " ['oklahomaok' 2]\n",
      " ['40pm' 2]\n",
      " ['pletchs' 2]\n",
      " ['roh3smantibatam' 2]\n",
      " ['smantibatam' 2]\n",
      " ['alrasyid448iturasya' 2]\n",
      " ['cadfyi' 2]\n",
      " ['mcgsecure' 2]\n",
      " ['liveleakfun' 2]\n",
      " ['notexplained' 2]\n",
      " ['softenza' 2]\n",
      " ['piner' 2]\n",
      " ['rdhorndale' 2]\n",
      " ['nuclearbiologicalchemical' 2]\n",
      " ['iredell' 2]\n",
      " ['072015' 2]\n",
      " ['fahlo' 2]\n",
      " ['sr14' 2]\n",
      " ['foxa' 2]\n",
      " ['cmcsa' 2]\n",
      " ['amcx' 2]\n",
      " ['viab' 2]\n",
      " ['twx' 2]\n",
      " ['ifak' 2]\n",
      " ['zabadani' 2]\n",
      " ['schwarber' 2]\n",
      " ['ahrar' 2]\n",
      " ['thisiswhywecanthavenicethings' 2]\n",
      " ['glink' 2]\n",
      " ['olhead' 2]\n",
      " ['marquei' 2]\n",
      " ['bancodeseries' 2]\n",
      " ['feelingmanly' 2]\n",
      " ['yiayplan' 2]\n",
      " ['japton' 2]\n",
      " ['wpri' 2]\n",
      " ['ttes' 2]\n",
      " ['quivk' 2]\n",
      " ['demolitiondodging' 2]\n",
      " ['enemity' 2]\n",
      " ['wftv' 2]\n",
      " ['redeemeth' 2]\n",
      " ['comingsoon' 2]\n",
      " ['shantae' 2]\n",
      " ['defundpp' 2]\n",
      " ['fotoset' 2]\n",
      " ['timelapse' 2]\n",
      " ['alcoholismaddiction' 2]\n",
      " ['gaelite' 2]\n",
      " ['gmmbc' 2]\n",
      " ['fifa16' 2]\n",
      " ['lv6' 2]\n",
      " ['motorcraft' 2]\n",
      " ['gawx' 2]\n",
      " ['ihhen' 2]\n",
      " ['nankana' 2]\n",
      " ['okthe' 2]\n",
      " ['considerelem' 2]\n",
      " ['trueheroes' 2]\n",
      " ['scwx' 2]\n",
      " ['pakpattan' 2]\n",
      " ['ks94' 2]\n",
      " ['wouldelectrocute' 2]\n",
      " ['emsc' 2]\n",
      " ['twotone' 2]\n",
      " ['crossbody' 2]\n",
      " ['renison' 2]\n",
      " ['cafire' 2]\n",
      " ['bago' 2]\n",
      " ['archipelagowolves' 2]\n",
      " ['kimery' 2]\n",
      " ['protectdenaliwolves' 2]\n",
      " ['msica' 2]\n",
      " ['jakartapost' 2]\n",
      " ['ti5' 2]\n",
      " ['wroug' 2]\n",
      " ['missionhills' 2]\n",
      " ['ivanberroa' 2]\n",
      " ['losdelsonido' 2]\n",
      " ['hammondville' 2]\n",
      " ['perrie' 2]\n",
      " ['hitchbot' 2]\n",
      " ['inec' 2]\n",
      " ['sensorknock' 2]\n",
      " ['sr91' 2]\n",
      " ['stagetwo' 2]\n",
      " ['jonathanferrell' 2]\n",
      " ['chesttorso' 2]\n",
      " ['albertans' 2]\n",
      " ['ef5' 2]\n",
      " ['wxky' 2]\n",
      " ['alwx' 2]\n",
      " ['askcharley' 2]\n",
      " ['raung' 2]\n",
      " ['monthold' 2]\n",
      " ['aogashima' 2]\n",
      " ['publichealth' 2]\n",
      " ['diaporama' 2]\n",
      " ['sixpenceee' 2]\n",
      " ['volcanoinrussia' 2]\n",
      " ['utc5' 2]\n",
      " ['insas' 2]\n",
      " ['us101' 2]\n",
      " ['stormchase' 2]\n",
      " ['cawx' 2]\n",
      " ['diageos' 2]\n",
      " ['thegame' 2]\n",
      " ['valleywx' 2]\n",
      " ['nycha' 2]\n",
      " ['memorialday' 2]\n",
      " ['pagasa' 2]\n",
      " ['lowlying' 2]\n",
      " ['worldvision' 2]\n",
      " ['jhaustin' 2]\n",
      " ['routecomplex' 2]\n",
      " ['outbid' 2]\n",
      " ['freefrom' 2]\n",
      " ['shania' 2]\n",
      " ['riversiskiyou' 2]\n",
      " ['dothraki' 2]\n",
      " ['seasonfrom' 2]\n",
      " ['onshit' 2]\n",
      " ['ushed' 2]\n",
      " ['whitbourne' 2]\n",
      " ['sn' 2]\n",
      " ['salvis' 2]\n",
      " ['windstorm' 2]\n",
      " ['achimota' 2]\n",
      " ['canberras' 2]\n",
      " ['tookem' 2]\n",
      " ['alllivesmatter' 2]\n",
      " ['kowing' 2]\n",
      " ['zippednews' 2]\n",
      " ['news3lv' 2]\n",
      " ['upwindstorm' 2]\n",
      " ['sixcar' 2]\n",
      " ['identitytheft' 2]\n",
      " ['justmarried' 2]\n",
      " ['vabengal' 2]\n",
      " ['irin' 2]\n",
      " ['easternoregon' 2]\n",
      " ['2676773' 2]\n",
      " ['mhtw4fnetcrews' 2]\n",
      " ['fingerrockfire' 2]\n",
      " ['njenga' 2]\n",
      " ['growingupblack' 2]\n",
      " ['birminghams' 2]\n",
      " ['unitedstates' 2]\n",
      " ['pawsox' 2]\n",
      " ['emergencyresponse' 2]\n",
      " ['lukebox' 2]\n",
      " ['infoorder' 2]\n",
      " ['sms087809233445' 2]\n",
      " ['insanelimits' 2]\n",
      " ['beclearoncancer' 2]\n",
      " ['engvaus' 2]\n",
      " ['lavenderpoetrycafe' 2]\n",
      " ['daytoday' 2]\n",
      " ['yakub' 2]\n",
      " ['rpics' 2]\n",
      " ['nasahurricane' 2]\n",
      " ['02pm' 2]\n",
      " ['saumur' 2]\n",
      " ['icaseit' 2]\n",
      " ['vibez' 2]\n",
      " ['dealsuk' 2]\n",
      " ['win10' 2]\n",
      " ['magner' 2]\n",
      " ['m151a1' 2]\n",
      " ['saltriverwildhorses' 2]\n",
      " ['ovofest' 2]\n",
      " ['bodybagging' 2]]\n"
     ]
    }
   ],
   "source": [
    "print(oov_df.head(500).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of tokens oov:  0.04076309050291505\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of tokens oov: \", \n",
    "      oov_df.number.sum() / (train['nlp'].apply(len).sum() + test['nlp'].apply(len).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To explore: use wordninja to cut up composite words/hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['typhoon', 'devastated']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('typhoondevastated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mh', '370']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('mh370')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre', 'break']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('prebreak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def get_X(df):\n",
    "    X = []\n",
    "    X_ext = []\n",
    "    xcols = ['has_location', 'geocoded','longitude_n','latitude_n','num_hash_n','num_mention_n','num_url_n']\n",
    "    for index, row in df.iterrows():\n",
    "        x = row['wordvec']\n",
    "        #x = numpy.append(x, row['keyword_wordvec'])\n",
    "        \n",
    "        X.append(x)\n",
    "        for xc in xcols:\n",
    "            x = numpy.append(x, row[xc])        \n",
    "        X_ext.append(x)\n",
    "    return X, X_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_ext = get_X(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svm = SVC(kernel=\"rbf\")\n",
    "params = {'C': [0.05, 0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 4]}\n",
    "clf = GridSearchCV(svm, params, scoring=\"f1\", verbose=1, n_jobs=-2, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([102.75017309, 111.90411711, 129.97967076]),\n",
       " 'score_time': array([3.09891152, 2.84578514, 3.09524584]),\n",
       " 'test_score': array([0.73401397, 0.7159035 , 0.78023033]),\n",
       " 'train_score': array([0.79595449, 0.81373044, 0.80443548])}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(clf, X, y, cv=3, return_train_score=True, scoring='f1')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7433825976979325"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(model, X, y, X_test, name):\n",
    "    fit = model.fit(X,y)\n",
    "    pred = model.predict(X_test)\n",
    "    submission = pd.DataFrame({\"id\":test['id'], \"target\":pred})\n",
    "    submission.to_csv(name+'.csv', index=False)\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  45 out of  45 | elapsed:  4.4min finished\n"
     ]
    }
   ],
   "source": [
    "X_test, X_test_ext = get_X(test)\n",
    "fit = prepare_submission(clf, X, y, X_test, 'avg_wordvec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([16.2656002 , 16.06399727, 14.81328235, 13.62246323, 13.98820539,\n",
       "        13.89963684, 13.16968436, 12.98290372, 13.06896777]),\n",
       " 'std_fit_time': array([1.16269804, 0.50647231, 0.76079187, 1.23859031, 0.82218409,\n",
       "        0.2663644 , 0.57257947, 0.71957938, 1.06073153]),\n",
       " 'mean_score_time': array([3.66964693, 3.62519808, 3.01858544, 2.94939013, 2.89965215,\n",
       "        2.81909885, 2.7464879 , 2.89028668, 2.63742886]),\n",
       " 'std_score_time': array([0.52700126, 0.33103732, 0.29818703, 0.29587163, 0.23702686,\n",
       "        0.22736502, 0.31430439, 0.32260753, 0.297495  ]),\n",
       " 'param_C': masked_array(data=[0.05, 0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.05},\n",
       "  {'C': 0.1},\n",
       "  {'C': 0.2},\n",
       "  {'C': 0.5},\n",
       "  {'C': 0.75},\n",
       "  {'C': 1},\n",
       "  {'C': 1.5},\n",
       "  {'C': 2},\n",
       "  {'C': 4}],\n",
       " 'split0_test_score': array([0.71715328, 0.72760181, 0.7277677 , 0.72661871, 0.7264574 ,\n",
       "        0.72987478, 0.72922252, 0.730187  , 0.72759539]),\n",
       " 'split1_test_score': array([0.69625762, 0.70232959, 0.71012007, 0.71864407, 0.72512648,\n",
       "        0.72527473, 0.71959459, 0.71212121, 0.70315091]),\n",
       " 'split2_test_score': array([0.71896552, 0.7306712 , 0.73781513, 0.7442623 , 0.74104235,\n",
       "        0.74350649, 0.74097835, 0.74221868, 0.73659306]),\n",
       " 'split3_test_score': array([0.6972639 , 0.70680628, 0.71242398, 0.70199826, 0.70408163,\n",
       "        0.69811321, 0.69699571, 0.69819433, 0.69439728]),\n",
       " 'split4_test_score': array([0.76767677, 0.76705491, 0.77435065, 0.77804681, 0.78162772,\n",
       "        0.78205128, 0.78691141, 0.78281623, 0.77647059]),\n",
       " 'mean_test_score': array([0.71946342, 0.72689276, 0.7324955 , 0.73391403, 0.73566711,\n",
       "        0.7357641 , 0.73474052, 0.73310749, 0.72764145]),\n",
       " 'std_test_score': array([0.02592852, 0.02295471, 0.02326383, 0.02592219, 0.02582571,\n",
       "        0.02743993, 0.02981688, 0.02905806, 0.02887691]),\n",
       " 'rank_test_score': array([9, 8, 6, 4, 2, 1, 3, 5, 7], dtype=int32)}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
