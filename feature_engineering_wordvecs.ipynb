{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate wordvecs for the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...  \n",
       "1       1             Forest fire near La Ronge Sask. Canada  \n",
       "2       1  All residents asked to 'shelter in place' are ...  \n",
       "3       1  people receive wildfires evacuation orders in ...  \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "train['cleaned_text'] = train['cleaned_text'].str.translate(table).str.strip()\n",
    "test['cleaned_text'] = test['cleaned_text'].str.translate(table).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(280):\n",
    "    train['cleaned_text'] = train['cleaned_text'].str.replace('  ', ' ')\n",
    "    test['cleaned_text'] = test['cleaned_text'].str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert keywords into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].fillna('',inplace=True)\n",
    "test['keyword'].fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_text_keyword'] = (train['keyword'] + ' ' + train['cleaned_text']).str.strip()\n",
    "test['cleaned_text_keyword'] = (test['keyword'] + ' ' + test['cleaned_text']).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nlp'] = train['cleaned_text'].apply(lambda s: nlp(s))\n",
    "train['wordvec'] = train['nlp'].apply(lambda s: s.vector)\n",
    "test['nlp'] = test['cleaned_text'].apply(lambda s: nlp(s))\n",
    "test['wordvec'] = test['nlp'].apply(lambda s: s.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword_nlp'] = train['cleaned_text_keyword'].apply(lambda s:nlp(s))\n",
    "train['keyword_wordvec'] = train['keyword_nlp'].apply(lambda s: s.vector)\n",
    "test['keyword_nlp'] = test['cleaned_text_keyword'].apply(lambda s:nlp(s))\n",
    "test['keyword_wordvec'] = test['keyword_nlp'].apply(lambda s: s.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the generate wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(df, i):\n",
    "    print(train['text'].iloc[i])\n",
    "    print(train['cleaned_text'].iloc[i])\n",
    "    for token in train['nlp'].iloc[i]:\n",
    "        print(token, token.has_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest fire near La Ronge Sask. Canada\n",
      "Forest fire near La Ronge Sask Canada\n",
      "Forest True\n",
      "fire True\n",
      "near True\n",
      "La True\n",
      "Ronge False\n",
      "Sask True\n",
      "Canada True\n"
     ]
    }
   ],
   "source": [
    "check(train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion https://t.co/gFJfgTodad\n",
      "Bahrain police had previously died in a road accident they were not killed by explosion\n",
      "Bahrain True\n",
      "police True\n",
      "had True\n",
      "previously True\n",
      "died True\n",
      "in True\n",
      "a True\n",
      "road True\n",
      "accident True\n",
      "they True\n",
      "were True\n",
      "not True\n",
      "killed True\n",
      "by True\n",
      "explosion True\n"
     ]
    }
   ],
   "source": [
    "check(train, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TradCatKnight (1) Russia may have played into reason but that link is BS.  Okanowa was bloody and mainline invasion looked like a bloody\n",
      "1 Russia may have played into reason but that link is BS Okanowa was bloody and mainline invasion looked like a bloody\n",
      "1 True\n",
      "Russia True\n",
      "may True\n",
      "have True\n",
      "played True\n",
      "into True\n",
      "reason True\n",
      "but True\n",
      "that True\n",
      "link True\n",
      "is True\n",
      "BS True\n",
      "Okanowa False\n",
      "was True\n",
      "bloody True\n",
      "and True\n",
      "mainline True\n",
      "invasion True\n",
      "looked True\n",
      "like True\n",
      "a True\n",
      "bloody True\n"
     ]
    }
   ],
   "source": [
    "check(train, 888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tokens = dict()\n",
    "\n",
    "def gather_tokens(oov_tokens, doc):\n",
    "    for token in doc:\n",
    "        if token.is_oov:\n",
    "            if str(token).lower() in oov_tokens:\n",
    "                oov_tokens[str(token).lower()] += 1\n",
    "            else:\n",
    "                oov_tokens[str(token).lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "3258    None\n",
       "3259    None\n",
       "3260    None\n",
       "3261    None\n",
       "3262    None\n",
       "Name: nlp, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))\n",
    "test['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_df = pd.DataFrame({'token':list(oov_tokens.keys()), 'number':list(oov_tokens.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mh370' 94]\n",
      " ['prebreak' 41]\n",
      " ['typhoondevastated' 32]\n",
      " ['soudelor' 28]\n",
      " ['funtenna' 26]\n",
      " ['disea' 25]\n",
      " ['gbbo' 23]\n",
      " ['udhampur' 21]\n",
      " ['bayelsa' 21]\n",
      " ['marians' 20]\n",
      " ['enugu' 19]\n",
      " ['utc20150805' 17]\n",
      " ['sensorsenso' 17]\n",
      " ['gtgt' 16]\n",
      " ['selfimage' 16]\n",
      " ['spos' 15]\n",
      " ['time20150806' 14]\n",
      " ['mtvhottest' 13]\n",
      " ['abstorm' 13]\n",
      " ['sismo' 13]\n",
      " ['bestnaijamade' 12]\n",
      " ['mediterran' 12]\n",
      " ['hwo' 11]\n",
      " ['irandeal' 11]\n",
      " ['linkury' 11]\n",
      " ['trfc' 11]\n",
      " ['okwx' 10]\n",
      " ['beyhive' 10]\n",
      " ['o784' 10]\n",
      " ['meatloving' 10]\n",
      " ['yazidis' 10]\n",
      " ['wheavenly' 10]\n",
      " ['sinjar' 10]\n",
      " ['yearold' 10]\n",
      " ['summerfate' 10]\n",
      " ['inj' 9]\n",
      " ['tubestrike' 9]\n",
      " ['chicagoarea' 9]\n",
      " ['breakingnews' 9]\n",
      " ['runion' 9]\n",
      " ['animalrescue' 8]\n",
      " ['trapmusic' 8]\n",
      " ['icemoon' 8]\n",
      " ['igers' 8]\n",
      " ['olap' 8]\n",
      " ['mansehra' 7]\n",
      " ['twia' 7]\n",
      " ['waterresistant' 7]\n",
      " ['explosionproof' 7]\n",
      " ['30pm' 7]\n",
      " ['pantherattack' 7]\n",
      " ['bb17' 7]\n",
      " ['zouma' 7]\n",
      " ['wisenews' 7]\n",
      " ['kisii' 6]\n",
      " ['strategicpatience' 6]\n",
      " ['auspol' 6]\n",
      " ['yycstorm' 6]\n",
      " ['abbswinston' 6]\n",
      " ['gtgtgt' 6]\n",
      " ['kerricktrial' 6]\n",
      " ['lglorg' 6]\n",
      " ['blksamp8whts' 6]\n",
      " ['hostageamp2' 6]\n",
      " ['wbioterrorismampuse' 6]\n",
      " ['saddlebrooke' 6]\n",
      " ['greeces' 6]\n",
      " ['rohingya' 6]\n",
      " ['idis' 6]\n",
      " ['playthursdays' 6]\n",
      " ['bigamist' 6]\n",
      " ['fergusons' 6]\n",
      " ['ramag' 6]\n",
      " ['lulgzimbestpicts' 6]\n",
      " ['listenbuy' 5]\n",
      " ['godslove' 5]\n",
      " ['sittwe' 5]\n",
      " ['gopdebate' 5]\n",
      " ['fettilootch' 5]\n",
      " ['detectado' 5]\n",
      " ['collisionno' 5]\n",
      " ['waimate' 5]\n",
      " ['blowmandyup' 5]\n",
      " ['52015' 5]\n",
      " ['yobe' 5]\n",
      " ['slanglucci' 5]\n",
      " ['beckarnley' 5]\n",
      " ['rockyfire' 5]\n",
      " ['otrametlife' 5]\n",
      " ['pdx911' 5]\n",
      " ['idfire' 5]\n",
      " ['naved' 5]\n",
      " ['metrofmtalk' 5]\n",
      " ['socialnews' 5]\n",
      " ['japn' 5]\n",
      " ['injuryi495' 5]\n",
      " ['i77' 5]\n",
      " ['cecilthelion' 5]\n",
      " ['earthquake' 5]\n",
      " ['bioterrorism' 5]\n",
      " ['thanku' 5]\n",
      " ['weallheartonedirection' 4]\n",
      " ['shelli' 4]\n",
      " ['climatechange' 4]\n",
      " ['time20150805' 4]\n",
      " ['sosfam' 4]\n",
      " ['wedaug5th' 4]\n",
      " ['wedn' 4]\n",
      " ['tripledigit' 4]\n",
      " ['connectorconnecto' 4]\n",
      " ['bb4sp' 4]\n",
      " ['karymsky' 4]\n",
      " ['beforeitsnews' 4]\n",
      " ['tasmanias' 4]\n",
      " ['rapidcity' 4]\n",
      " ['pbban' 4]\n",
      " ['temporary300' 4]\n",
      " ['ultimalucha' 4]\n",
      " ['armageddon' 4]\n",
      " ['humaza' 4]\n",
      " ['ccot' 4]\n",
      " ['sa15' 4]\n",
      " ['demonstratio' 4]\n",
      " ['fennovoima' 4]\n",
      " ['soudelors' 4]\n",
      " ['worstsummerjob' 4]\n",
      " ['isil' 4]\n",
      " ['standwithpp' 4]\n",
      " ['29072015' 4]\n",
      " ['silvergray' 4]\n",
      " ['votejkt48id' 4]\n",
      " ['gunsense' 4]\n",
      " ['hannemans' 4]\n",
      " ['talkradio' 4]\n",
      " ['daesh' 4]\n",
      " ['idps' 4]\n",
      " ['southdowns' 4]\n",
      " ['nikeplus' 4]\n",
      " ['modiministry' 4]\n",
      " ['sigalert' 4]\n",
      " ['m194' 4]\n",
      " ['roh3' 4]\n",
      " ['blacklivesmatter' 4]\n",
      " ['nineyearold' 4]\n",
      " ['massmurderer' 4]\n",
      " ['reaad' 4]\n",
      " ['prophetmuhammad' 4]\n",
      " ['hatcap' 4]\n",
      " ['hiroshima70' 3]\n",
      " ['dorrets' 3]\n",
      " ['globalwarming' 3]\n",
      " ['indiannews' 3]\n",
      " ['plannedparenthood' 3]\n",
      " ['dorret' 3]\n",
      " ['cbcca' 3]\n",
      " ['harda' 3]\n",
      " ['tornadogiveaway' 3]\n",
      " ['15pm' 3]\n",
      " ['kotaweather' 3]\n",
      " ['greatbritishbakeoff' 3]\n",
      " ['topstories' 3]\n",
      " ['fieg' 3]\n",
      " ['rworldnews' 3]\n",
      " ['collisionunkn' 3]\n",
      " ['luchaunderground' 3]\n",
      " ['tcot' 3]\n",
      " ['bioterror' 3]\n",
      " ['rickperry' 3]\n",
      " ['slosher' 3]\n",
      " ['bookboost' 3]\n",
      " ['faroeislands' 3]\n",
      " ['debatequestionswewanttohear' 3]\n",
      " ['ibooklove' 3]\n",
      " ['aoms' 3]\n",
      " ['icymi' 3]\n",
      " ['gtii' 3]\n",
      " ['bakeofffriends' 3]\n",
      " ['i405' 3]\n",
      " ['askconnor' 3]\n",
      " ['treeporn' 3]\n",
      " ['njturnpike' 3]\n",
      " ['multidimensi' 3]\n",
      " ['throwingknifes' 3]\n",
      " ['humanconsumption' 3]\n",
      " ['magginoodle' 3]\n",
      " ['nestleindia' 3]\n",
      " ['offers2go' 3]\n",
      " ['reactorbased' 3]\n",
      " ['indojapan' 3]\n",
      " ['nucleardeal' 3]\n",
      " ['foodscare' 3]\n",
      " ['artistsunited' 3]\n",
      " ['linerless' 3]\n",
      " ['ekdar' 3]\n",
      " ['collision1141' 3]\n",
      " ['enrt' 3]\n",
      " ['politifiact' 3]\n",
      " ['sixmeter' 3]\n",
      " ['harrybecareful' 3]\n",
      " ['magnetraction' 3]\n",
      " ['hempoil' 3]\n",
      " ['standuser' 3]\n",
      " ['haiyan' 3]\n",
      " ['gilbert23' 3]\n",
      " ['commoditiesare' 3]\n",
      " ['calgarys' 3]\n",
      " ['latestnews' 3]\n",
      " ['followme' 3]\n",
      " ['exp0sed' 3]\n",
      " ['festac' 3]\n",
      " ['s3xleakph0tos' 3]\n",
      " ['bacup' 3]\n",
      " ['literarycakes' 3]\n",
      " ['bluedio' 3]\n",
      " ['hannaph' 3]\n",
      " ['soudelortropical' 3]\n",
      " ['propertycasualty' 3]\n",
      " ['grupdates' 3]\n",
      " ['abuseddesolateamplost' 3]\n",
      " ['whelen' 3]\n",
      " ['letsfootball' 3]\n",
      " ['hermancranston' 3]\n",
      " ['undergroundrailraod' 3]\n",
      " ['offensivecontent' 3]\n",
      " ['arwx' 3]\n",
      " ['yugvani' 3]\n",
      " ['onlinecommunities' 3]\n",
      " ['tilnow' 3]\n",
      " ['noonanheartbreak' 3]\n",
      " ['yahistorical' 3]\n",
      " ['sportwatch' 3]\n",
      " ['ss100' 3]\n",
      " ['bhramabull' 3]\n",
      " ['gamergate' 3]\n",
      " ['uptotheminute' 3]\n",
      " ['clearedincident' 3]\n",
      " ['apch' 3]\n",
      " ['oper' 3]\n",
      " ['navbl' 3]\n",
      " ['warningissued' 3]\n",
      " ['efak' 3]\n",
      " ['liveonk2' 3]\n",
      " ['mido' 3]\n",
      " ['putins' 3]\n",
      " ['earththus' 3]\n",
      " ['bankstown' 3]\n",
      " ['versethe' 3]\n",
      " ['mhtw4fnetofficials' 3]\n",
      " ['whatsmentioned' 3]\n",
      " ['thebeginning' 3]\n",
      " ['littledeath' 3]\n",
      " ['ruebs' 3]\n",
      " ['rightways' 3]\n",
      " ['harmkid' 3]\n",
      " ['videoveranomtv' 3]\n",
      " ['photoset' 3]\n",
      " ['mmda' 3]\n",
      " ['tgf2015' 3]\n",
      " ['savebees' 3]\n",
      " ['newsintweets' 3]\n",
      " ['listenlive' 3]\n",
      " ['usnwsgov' 3]\n",
      " ['salopek' 3]\n",
      " ['abomb' 3]\n",
      " ['diesis' 3]\n",
      " ['banki' 3]\n",
      " ['pjnet' 3]\n",
      " ['koin6news' 3]\n",
      " ['buddys' 3]\n",
      " ['calgaryweather' 2]\n",
      " ['newsdict' 2]\n",
      " ['i580' 2]\n",
      " ['stagefright' 2]\n",
      " ['jamaicaplain' 2]\n",
      " ['ofclans' 2]\n",
      " ['buffetts' 2]\n",
      " ['offr' 2]\n",
      " ['eightynine' 2]\n",
      " ['ptsdchat' 2]\n",
      " ['sr37' 2]\n",
      " ['mi17' 2]\n",
      " ['twovehicle' 2]\n",
      " ['yycweather' 2]\n",
      " ['mhtw4fnetpakistan' 2]\n",
      " ['traumatised' 2]\n",
      " ['pcps' 2]\n",
      " ['muzzamil' 2]\n",
      " ['38pm' 2]\n",
      " ['techesback' 2]\n",
      " ['skanndtyagi' 2]\n",
      " ['carryi' 2]\n",
      " ['yazidi' 2]\n",
      " ['janaq' 2]\n",
      " ['meteoearth' 2]\n",
      " ['angelriveralib' 2]\n",
      " ['buildingswe' 2]\n",
      " ['dhsscitech' 2]\n",
      " ['forestservice' 2]\n",
      " ['lightningcaused' 2]\n",
      " ['fatburning' 2]\n",
      " ['renew911health' 2]\n",
      " ['threealarm' 2]\n",
      " ['pulwama' 2]\n",
      " ['wmur9' 2]\n",
      " ['autoinsurance' 2]\n",
      " ['22pm' 2]\n",
      " ['tweetlikeitsseptember11th2001' 2]\n",
      " ['cbplawyers' 2]\n",
      " ['homerescuers' 2]\n",
      " ['strikesstrikes' 2]\n",
      " ['newsmornings' 2]\n",
      " ['oworoshoki' 2]\n",
      " ['bstop' 2]\n",
      " ['urgentthere' 2]\n",
      " ['antiterrorism' 2]\n",
      " ['tlvfaces' 2]\n",
      " ['zarry' 2]\n",
      " ['internallydisplaced' 2]\n",
      " ['c4news' 2]\n",
      " ['eudrylantiqua' 2]\n",
      " ['capsizes' 2]\n",
      " ['walerga' 2]\n",
      " ['lasvegas' 2]\n",
      " ['wdsu' 2]\n",
      " ['ptbo' 2]\n",
      " ['newswatch' 2]\n",
      " ['ebike' 2]\n",
      " ['thebookclub' 2]\n",
      " ['oklahomaok' 2]\n",
      " ['40pm' 2]\n",
      " ['pletchs' 2]\n",
      " ['roh3smantibatam' 2]\n",
      " ['smantibatam' 2]\n",
      " ['alrasyid448iturasya' 2]\n",
      " ['cadfyi' 2]\n",
      " ['mcgsecure' 2]\n",
      " ['liveleakfun' 2]\n",
      " ['notexplained' 2]\n",
      " ['softenza' 2]\n",
      " ['piner' 2]\n",
      " ['rdhorndale' 2]\n",
      " ['nuclearbiologicalchemical' 2]\n",
      " ['iredell' 2]\n",
      " ['072015' 2]\n",
      " ['fahlo' 2]\n",
      " ['sr14' 2]\n",
      " ['foxa' 2]\n",
      " ['cmcsa' 2]\n",
      " ['amcx' 2]\n",
      " ['viab' 2]\n",
      " ['twx' 2]\n",
      " ['ifak' 2]\n",
      " ['zabadani' 2]\n",
      " ['schwarber' 2]\n",
      " ['ahrar' 2]\n",
      " ['thisiswhywecanthavenicethings' 2]\n",
      " ['glink' 2]\n",
      " ['olhead' 2]\n",
      " ['marquei' 2]\n",
      " ['bancodeseries' 2]\n",
      " ['feelingmanly' 2]\n",
      " ['yiayplan' 2]\n",
      " ['japton' 2]\n",
      " ['wpri' 2]\n",
      " ['ttes' 2]\n",
      " ['quivk' 2]\n",
      " ['demolitiondodging' 2]\n",
      " ['enemity' 2]\n",
      " ['wftv' 2]\n",
      " ['redeemeth' 2]\n",
      " ['comingsoon' 2]\n",
      " ['shantae' 2]\n",
      " ['defundpp' 2]\n",
      " ['fotoset' 2]\n",
      " ['timelapse' 2]\n",
      " ['alcoholismaddiction' 2]\n",
      " ['gaelite' 2]\n",
      " ['gmmbc' 2]\n",
      " ['fifa16' 2]\n",
      " ['lv6' 2]\n",
      " ['motorcraft' 2]\n",
      " ['gawx' 2]\n",
      " ['ihhen' 2]\n",
      " ['nankana' 2]\n",
      " ['okthe' 2]\n",
      " ['considerelem' 2]\n",
      " ['trueheroes' 2]\n",
      " ['scwx' 2]\n",
      " ['pakpattan' 2]\n",
      " ['ks94' 2]\n",
      " ['wouldelectrocute' 2]\n",
      " ['emsc' 2]\n",
      " ['twotone' 2]\n",
      " ['crossbody' 2]\n",
      " ['renison' 2]\n",
      " ['cafire' 2]\n",
      " ['bago' 2]\n",
      " ['archipelagowolves' 2]\n",
      " ['kimery' 2]\n",
      " ['protectdenaliwolves' 2]\n",
      " ['msica' 2]\n",
      " ['jakartapost' 2]\n",
      " ['ti5' 2]\n",
      " ['wroug' 2]\n",
      " ['missionhills' 2]\n",
      " ['ivanberroa' 2]\n",
      " ['losdelsonido' 2]\n",
      " ['hammondville' 2]\n",
      " ['perrie' 2]\n",
      " ['hitchbot' 2]\n",
      " ['inec' 2]\n",
      " ['sensorknock' 2]\n",
      " ['sr91' 2]\n",
      " ['stagetwo' 2]\n",
      " ['jonathanferrell' 2]\n",
      " ['chesttorso' 2]\n",
      " ['albertans' 2]\n",
      " ['ef5' 2]\n",
      " ['wxky' 2]\n",
      " ['alwx' 2]\n",
      " ['askcharley' 2]\n",
      " ['raung' 2]\n",
      " ['monthold' 2]\n",
      " ['aogashima' 2]\n",
      " ['publichealth' 2]\n",
      " ['diaporama' 2]\n",
      " ['sixpenceee' 2]\n",
      " ['volcanoinrussia' 2]\n",
      " ['utc5' 2]\n",
      " ['insas' 2]\n",
      " ['us101' 2]\n",
      " ['stormchase' 2]\n",
      " ['cawx' 2]\n",
      " ['diageos' 2]\n",
      " ['thegame' 2]\n",
      " ['valleywx' 2]\n",
      " ['nycha' 2]\n",
      " ['memorialday' 2]\n",
      " ['pagasa' 2]\n",
      " ['lowlying' 2]\n",
      " ['worldvision' 2]\n",
      " ['jhaustin' 2]\n",
      " ['routecomplex' 2]\n",
      " ['outbid' 2]\n",
      " ['freefrom' 2]\n",
      " ['shania' 2]\n",
      " ['riversiskiyou' 2]\n",
      " ['dothraki' 2]\n",
      " ['seasonfrom' 2]\n",
      " ['onshit' 2]\n",
      " ['ushed' 2]\n",
      " ['whitbourne' 2]\n",
      " ['sn' 2]\n",
      " ['salvis' 2]\n",
      " ['windstorm' 2]\n",
      " ['achimota' 2]\n",
      " ['canberras' 2]\n",
      " ['tookem' 2]\n",
      " ['alllivesmatter' 2]\n",
      " ['kowing' 2]\n",
      " ['zippednews' 2]\n",
      " ['news3lv' 2]\n",
      " ['upwindstorm' 2]\n",
      " ['sixcar' 2]\n",
      " ['identitytheft' 2]\n",
      " ['justmarried' 2]\n",
      " ['vabengal' 2]\n",
      " ['irin' 2]\n",
      " ['easternoregon' 2]\n",
      " ['2676773' 2]\n",
      " ['mhtw4fnetcrews' 2]\n",
      " ['fingerrockfire' 2]\n",
      " ['njenga' 2]\n",
      " ['growingupblack' 2]\n",
      " ['birminghams' 2]\n",
      " ['unitedstates' 2]\n",
      " ['pawsox' 2]\n",
      " ['emergencyresponse' 2]\n",
      " ['lukebox' 2]\n",
      " ['infoorder' 2]\n",
      " ['sms087809233445' 2]\n",
      " ['insanelimits' 2]\n",
      " ['beclearoncancer' 2]\n",
      " ['engvaus' 2]\n",
      " ['lavenderpoetrycafe' 2]\n",
      " ['daytoday' 2]\n",
      " ['yakub' 2]\n",
      " ['rpics' 2]\n",
      " ['nasahurricane' 2]\n",
      " ['02pm' 2]\n",
      " ['saumur' 2]\n",
      " ['icaseit' 2]\n",
      " ['vibez' 2]\n",
      " ['dealsuk' 2]\n",
      " ['win10' 2]\n",
      " ['magner' 2]\n",
      " ['m151a1' 2]\n",
      " ['saltriverwildhorses' 2]\n",
      " ['ovofest' 2]\n",
      " ['bodybagging' 2]]\n"
     ]
    }
   ],
   "source": [
    "oov_df = oov_df.sort_values(by='number',ascending=False)\n",
    "print(oov_df.head(500).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of tokens oov:  0.04076309050291505\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of tokens oov: \", \n",
    "      oov_df.number.sum() / (train['nlp'].apply(len).sum() + test['nlp'].apply(len).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate concatenated wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 3), dtype=float64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = numpy.empty((0,3))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = numpy.append(t, [[1,2,3]],axis=0)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = numpy.append(t,[[4,5,6]],axis=0)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.],\n",
       "       [7., 8., 9.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.append(t,[[7,8,9]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(doc):\n",
    "    concat = numpy.empty((0,300))\n",
    "    for token in doc:\n",
    "        concat = numpy.append(concat, [token.vector], axis = 0)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['wordvec_concat'] = train['nlp'].apply(lambda s: concatenate(s))\n",
    "test['wordvec_concat'] = test['nlp'].apply(lambda s: concatenate(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 300)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['wordvec_concat'][10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated averaged wordvecs using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train['cleaned_text'].append(test['cleaned_text'],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "vectorizer.fit_transform(sentences)\n",
    "idf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def get_tfidf_mean(doc):\n",
    "    avg = numpy.zeros(len(doc.vector))\n",
    "    n = 0\n",
    "    for token in doc:        \n",
    "        if not token.is_oov:\n",
    "            token_str = str(token).lower()\n",
    "            if token_str in idf:\n",
    "                avg += token.vector * idf[token_str]\n",
    "                n += 1\n",
    "    if n>0:\n",
    "        avg = avg/n\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['wordvec_tfidf'] = train['nlp'].apply(lambda s: get_tfidf_mean(s))\n",
    "test['wordvec_tfidf'] = test['nlp'].apply(lambda s: get_tfidf_mean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-2.0410312242232838, 0.1577752003302941, -0.8...\n",
       "1       [-0.27185601989428204, 0.2042857458194097, -1....\n",
       "2       [0.07528745450756767, 0.11175614595413208, -0....\n",
       "3       [-1.3403782035623277, 1.2000715562275477, 0.11...\n",
       "4       [-0.7245167245467504, -0.364056259393692, 0.52...\n",
       "                              ...                        \n",
       "7556    [0.8706045945485433, -1.4970579627487395, -1.5...\n",
       "7557    [-0.4535740460786555, 0.6074934605922964, 0.17...\n",
       "7558    [0.804118087887764, 0.12538206577301025, 1.120...\n",
       "7559    [0.3517053962192115, 0.9118178826482857, -1.03...\n",
       "7560    [-0.40453157777136023, 1.0456219220703298, 0.1...\n",
       "Name: wordvec_tfidf, Length: 7561, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['wordvec_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-0.26623327, 0.05843069, -0.1404636, -0.05265...\n",
       "1       [-0.025449565, 0.031005142, -0.15566371, -0.23...\n",
       "2       [0.0059339865, 0.016337818, -0.105279535, -0.0...\n",
       "3       [-0.18147185, 0.20731743, 0.014147284, -0.2182...\n",
       "4       [-0.06394094, -0.01423019, 0.0063574947, 0.071...\n",
       "                              ...                        \n",
       "7556    [0.1382938, -0.1897513, -0.23208952, 0.0764361...\n",
       "7557    [-0.035660267, 0.12369229, 0.0052933893, -0.09...\n",
       "7558    [0.10752811, 0.074644, 0.046825, -0.2527535, 0...\n",
       "7559    [0.03056033, 0.15288134, -0.13398015, 0.086398...\n",
       "7560    [-0.08448874, 0.18175545, 0.011566181, -0.1192...\n",
       "Name: wordvec, Length: 7561, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['wordvec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To explore: use wordninja to cut up composite words/hashtags\n",
    "Several OOV words seem to be composites that could be cut up. Wordninja can do this. For now just leave as is, since it's only 4% of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['typhoon', 'devastated']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('typhoondevastated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mh', '370']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('mh370')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre', 'break']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('prebreak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following wordvec data is produced:\n",
    "* wordvec: wordvec from the tweet (average of the wordvec of all words in the tweet)\n",
    "* keyword_wordvec: wordvec from keyword_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_save = ['id', 'wordvec', 'keyword_wordvec', 'wordvec_concat', 'wordvec_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_save].to_pickle('train_wordvec.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[columns_to_save].to_pickle('test_wordvec.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
