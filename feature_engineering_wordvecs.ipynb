{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate wordvecs for the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>glove_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>our deeds are the reason of this &lt;hashtag&gt; ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>&lt;number&gt; people receive &lt;hashtag&gt; wildfires ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>just got sent this photo from ruby &lt;hashtag&gt; a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1       1             Forest fire near La Ronge Sask. Canada   \n",
       "2       1  All residents asked to 'shelter in place' are ...   \n",
       "3       1  people receive wildfires evacuation orders in ...   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                  glove_cleaned_text  \n",
       "0  our deeds are the reason of this <hashtag> ear...  \n",
       "1             forest fire near la ronge sask. canada  \n",
       "2  all residents asked to 'shelter in place' are ...  \n",
       "3  <number> people receive <hashtag> wildfires ev...  \n",
       "4  just got sent this photo from ruby <hashtag> a...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"wordvecs/glove.twitter.27B.25d\"\n",
    "MODEL_NAME = \"glove.twitter.27B.25d\"\n",
    "if MODEL_NAME.startswith('glove.'):\n",
    "    TEXT_COL = 'glove_cleaned_text'\n",
    "else:\n",
    "    TEXT_COL = 'cleaned_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load a larger model with vectors\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "import string\n",
    "TWITTER_TOKENS = ['<url>', '<user>', '<smile>', '<lolface>','<sadface>','<neutralface>',\n",
    "                 '<heart>','<number>','<allcaps>','<hashtag>','<repeat>','<elong>']\n",
    "\n",
    "if MODEL_NAME.startswith('glove.'):\n",
    "    for t in TWITTER_TOKENS:\n",
    "        nlp.tokenizer.add_special_case(t,[{ORTH: t}])\n",
    "else:\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    train['cleaned_text'] = train['cleaned_text'].str.translate(table).str.strip()\n",
    "    test['cleaned_text'] = test['cleaned_text'].str.translate(table).str.strip()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert keywords into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].fillna('',inplace=True)\n",
    "test['keyword'].fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_text_keyword'] = (train['keyword'] + ' ' + train[TEXT_COL]).str.strip()\n",
    "test['cleaned_text_keyword'] = (test['keyword'] + ' ' + test[TEXT_COL]).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nlp'] = train[TEXT_COL].apply(lambda s: nlp(s))\n",
    "train['wordvec'] = train['nlp'].apply(lambda s: s.vector)\n",
    "test['nlp'] = test[TEXT_COL].apply(lambda s: nlp(s))\n",
    "test['wordvec'] = test['nlp'].apply(lambda s: s.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword_nlp'] = train['cleaned_text_keyword'].apply(lambda s:nlp(s))\n",
    "train['keyword_wordvec'] = train['keyword_nlp'].apply(lambda s: s.vector)\n",
    "test['keyword_nlp'] = test['cleaned_text_keyword'].apply(lambda s:nlp(s))\n",
    "test['keyword_wordvec'] = test['keyword_nlp'].apply(lambda s: s.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the generate wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(df, i):\n",
    "    print(train['text'].iloc[i])\n",
    "    print(train[TEXT_COL].iloc[i])\n",
    "    for token in train['nlp'].iloc[i]:\n",
    "        print(token, token.has_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest fire near La Ronge Sask. Canada\n",
      "forest fire near la ronge sask. canada\n",
      "forest True\n",
      "fire True\n",
      "near True\n",
      "la True\n",
      "ronge True\n",
      "sask True\n",
      ". True\n",
      "canada True\n"
     ]
    }
   ],
   "source": [
    "check(train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion https://t.co/gFJfgTodad\n",
      ".<user> <hashtag> bahrain police had previously died in a road accident they were not killed by explosion <url>\n",
      ".<user False\n",
      "> True\n",
      "<hashtag> True\n",
      "bahrain True\n",
      "police True\n",
      "had True\n",
      "previously True\n",
      "died True\n",
      "in True\n",
      "a True\n",
      "road True\n",
      "accident True\n",
      "they True\n",
      "were True\n",
      "not True\n",
      "killed True\n",
      "by True\n",
      "explosion True\n",
      "<url> True\n"
     ]
    }
   ],
   "source": [
    "check(train, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TradCatKnight (1) Russia may have played into reason but that link is BS.  Okanowa was bloody and mainline invasion looked like a bloody\n",
      "<user> (<number>) russia may have played into reason but that link is bs <allcaps>. okanowa was bloody and mainline invasion looked like a bloody\n",
      "<user> True\n",
      "( True\n",
      "<number> True\n",
      ") True\n",
      "russia True\n",
      "may True\n",
      "have True\n",
      "played True\n",
      "into True\n",
      "reason True\n",
      "but True\n",
      "that True\n",
      "link True\n",
      "is True\n",
      "bs True\n",
      "<allcaps> True\n",
      ". True\n",
      "okanowa False\n",
      "was True\n",
      "bloody True\n",
      "and True\n",
      "mainline True\n",
      "invasion True\n",
      "looked True\n",
      "like True\n",
      "a True\n",
      "bloody True\n"
     ]
    }
   ],
   "source": [
    "check(train, 888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tokens = dict()\n",
    "\n",
    "def gather_tokens(oov_tokens, doc):\n",
    "    for token in doc:\n",
    "        if token.is_oov:\n",
    "            if str(token).lower() in oov_tokens:\n",
    "                oov_tokens[str(token).lower()] += 1\n",
    "            else:\n",
    "                oov_tokens[str(token).lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "3258    None\n",
       "3259    None\n",
       "3260    None\n",
       "3261    None\n",
       "3262    None\n",
       "Name: nlp, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))\n",
    "test['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_df = pd.DataFrame({'token':list(oov_tokens.keys()), 'number':list(oov_tokens.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['allcaps><number' 190]\n",
      " ['\\x89û' 87]\n",
      " ['\\x89ûò' 66]\n",
      " ['number>%' 58]\n",
      " ['number><number' 54]\n",
      " ['.<user' 45]\n",
      " ['bioterror' 42]\n",
      " ['\\x89ûó' 42]\n",
      " ['prebreak' 41]\n",
      " ['bioterrorism' 38]\n",
      " ['re\\x89û' 37]\n",
      " ['soudelor' 36]\n",
      " ['number><number><number' 34]\n",
      " ['number>-year' 28]\n",
      " ['funtenna' 26]\n",
      " ['disea' 25]\n",
      " ['udhampur' 22]\n",
      " ['\\x89ûï' 22]\n",
      " ['don\\x89ûªt' 21]\n",
      " ['\\x89ûïwhen' 18]\n",
      " ['allcaps><number><number><number' 18]\n",
      " ['crematoria' 17]\n",
      " ['number>+' 17]\n",
      " ['gt;&gt' 14]\n",
      " ['spos' 14]\n",
      " ['canaanites' 14]\n",
      " ['rea\\x89û' 14]\n",
      " ['repeat><hashtag' 13]\n",
      " ['inundation' 12]\n",
      " ['it\\x89ûªs' 12]\n",
      " ['bestnaijamade' 12]\n",
      " ['mediterran' 12]\n",
      " ['china\\x89ûªs' 11]\n",
      " ['i\\x89ûªm' 11]\n",
      " ['read\\x89û' 11]\n",
      " ['linkury' 11]\n",
      " ['mhtw' 11]\n",
      " ['fnet' 11]\n",
      " ['yazidis' 10]\n",
      " ['sinjar' 10]\n",
      " ['å£<number' 9]\n",
      " ['by\\x89û' 9]\n",
      " ['microlight' 9]\n",
      " ['rì' 9]\n",
      " ['lt;<number' 9]\n",
      " ['ices\\x89û' 8]\n",
      " ['\\x89û÷politics' 8]\n",
      " ['animalrescue' 8]\n",
      " [\"allcaps>'t\" 8]\n",
      " ['number>\\x89û' 8]\n",
      " ['icemoon' 8]\n",
      " ['grief\\x89ûª' 8]\n",
      " ['can\\x89ûªt' 7]\n",
      " ['america\\x89ûªs' 7]\n",
      " ['twia' 7]\n",
      " ['tab\\x89û' 7]\n",
      " ['--&gt' 7]\n",
      " [' ' 7]\n",
      " ['you\\x89ûªve' 7]\n",
      " ['rear-' 7]\n",
      " [':p' 7]\n",
      " ['just\\x89û' 6]\n",
      " ['let\\x89ûªs' 6]\n",
      " ['of\\x89û' 6]\n",
      " ['abbswinston' 6]\n",
      " ['saddlebrooke' 6]\n",
      " ['warfighting' 6]\n",
      " ['gt;&gt;&gt' 6]\n",
      " ['bigamist' 6]\n",
      " ['dorret' 6]\n",
      " ['the\\x89û' 6]\n",
      " ['allcaps><sadface' 6]\n",
      " ['lulgzimbestpicts' 6]\n",
      " ['åê' 6]\n",
      " ['here\\x89ûªs' 6]\n",
      " ['yycstorm' 6]\n",
      " ['.potus' 6]\n",
      " ['hostage&amp;<number' 6]\n",
      " ['allcaps>\\x89û' 6]\n",
      " ['blks&amp;<number' 6]\n",
      " ['bioterrorism&amp;use' 6]\n",
      " ['kerrick' 5]\n",
      " ['godslove' 5]\n",
      " ['oppressions' 5]\n",
      " ['japì_n' 5]\n",
      " ['number>-day' 5]\n",
      " ['waimate' 5]\n",
      " ['i\\x89ûªve' 5]\n",
      " ['number>\\x89ûò' 5]\n",
      " ['otrametlife' 5]\n",
      " ['åè' 5]\n",
      " ['s\\x89û' 5]\n",
      " ['sittwe' 5]\n",
      " ['socialnews' 5]\n",
      " ['slanglucci' 5]\n",
      " ['\\x89ûïwe' 5]\n",
      " ['user><user' 5]\n",
      " ['fettilootch' 5]\n",
      " ['\\x89û÷extremely' 5]\n",
      " ['offensive\\x89ûª' 5]\n",
      " ['humaza' 4]\n",
      " ['number>-d' 4]\n",
      " ['you\\x89ûªre' 4]\n",
      " ['reshapes' 4]\n",
      " ['allcaps>-' 4]\n",
      " ['\\x89ûïa' 4]\n",
      " ['t\\x89û' 4]\n",
      " ['australia\\x89ûªs' 4]\n",
      " ['rapidcity' 4]\n",
      " ['number>-alarm' 4]\n",
      " ['that\\x89û' 4]\n",
      " ['that\\x89ûªs' 4]\n",
      " ['number>-foot' 4]\n",
      " ['number>-inspired' 4]\n",
      " ['allcaps>\\x89ûªs' 4]\n",
      " ['\\x89ûïthe' 4]\n",
      " ['whelen' 4]\n",
      " ['å¨' 4]\n",
      " ['fennovoima' 4]\n",
      " ['mitt.\\x89û\\x9d' 4]\n",
      " ['i\\x89û' 4]\n",
      " [\"i'm\" 4]\n",
      " ['temporary:<number' 4]\n",
      " ['weallheartonedirection' 4]\n",
      " ['allcaps>.aug' 4]\n",
      " [\"user>'armageddon\" 4]\n",
      " ['a\\x89û' 4]\n",
      " ['beforeitsnews' 4]\n",
      " ['allcaps>;<number' 4]\n",
      " ['nike+' 4]\n",
      " ['demonstratio' 4]\n",
      " ['be\\x89û' 4]\n",
      " ['karymsky' 4]\n",
      " [\"number>'<number\" 4]\n",
      " ['wouldn\\x89ûªt' 4]\n",
      " ['local\\x89û' 4]\n",
      " ['naved' 4]\n",
      " ['ìñ' 4]\n",
      " ['salopek' 3]\n",
      " ['user>\\x89û' 3]\n",
      " ['b&gt' 3]\n",
      " ['user>-heartbreak' 3]\n",
      " ['istorical' 3]\n",
      " ['versethe' 3]\n",
      " ['railraod' 3]\n",
      " ['taxiways' 3]\n",
      " ['sgov' 3]\n",
      " ['hermancranston' 3]\n",
      " ['til_now' 3]\n",
      " ['yugvani' 3]\n",
      " ['~<number' 3]\n",
      " ['\\x89û¢' 3]\n",
      " ['refugees\\x89û' 3]\n",
      " ['xleak' 3]\n",
      " ['magginoodle' 3]\n",
      " ['humanconsumption' 3]\n",
      " ['allcaps>.ca' 3]\n",
      " ['in\\x89û' 3]\n",
      " ['number><smile' 3]\n",
      " ['ìàekdar' 3]\n",
      " ['wmur' 3]\n",
      " ['number>-month' 3]\n",
      " ['doesn\\x89ûªt' 3]\n",
      " ['number>;s' 3]\n",
      " ['throwingknifes' 3]\n",
      " ['-&gt' 3]\n",
      " ['wowo--===' 3]\n",
      " ['hempoil' 3]\n",
      " ['allcaps><hashtag' 3]\n",
      " ['number><user>:<number' 3]\n",
      " ['slosher' 3]\n",
      " ['rupdates' 3]\n",
      " ['migrants\\x89û' 3]\n",
      " ['efak' 3]\n",
      " ['allcaps>:incident' 3]\n",
      " ['h\\x89û\\x9d' 3]\n",
      " ['allcaps>-traction' 3]\n",
      " ['savebees' 3]\n",
      " ['linerless' 3]\n",
      " ['sitting\\x89û' 3]\n",
      " ['\\x89ûïyou' 3]\n",
      " ['allcaps>&amp;lost' 3]\n",
      " ['pin:<number' 3]\n",
      " ['foodscare' 3]\n",
      " ['control?\\x89û\\x9d' 3]\n",
      " ['\\x89ûïhatchet' 3]\n",
      " ['\\x89ûïrichmond' 3]\n",
      " ['allcaps>+desolate' 3]\n",
      " ['politifiact' 3]\n",
      " ['rightways' 3]\n",
      " [\"allcaps>'re\" 3]\n",
      " ['ibooklove' 3]\n",
      " ['number>-u.s' 3]\n",
      " ['h\\x89û' 3]\n",
      " ['navbl' 3]\n",
      " ['multidimensi' 3]\n",
      " ['treeporn' 3]\n",
      " ['at\\x89û' 3]\n",
      " ['allcaps>]?<number' 3]\n",
      " ['offensiveåêcontent' 3]\n",
      " ['bookboost' 3]\n",
      " ['commoditiesåêare' 3]\n",
      " ['bluedio' 3]\n",
      " ['number>-feet' 3]\n",
      " ['allcaps>-ii' 3]\n",
      " ['ruebs' 3]\n",
      " ['onlinecommunities' 3]\n",
      " ['k_matako_bot' 2]\n",
      " ['donå«t' 2]\n",
      " ['you\\x89ûªll' 2]\n",
      " ['shawie' 2]\n",
      " ['tarmineta' 2]\n",
      " ['dambisa' 2]\n",
      " ['sanitised' 2]\n",
      " ['amcx' 2]\n",
      " ['viab' 2]\n",
      " ['schedule\\x89û' 2]\n",
      " ['city&amp;<number' 2]\n",
      " ['number>-pin' 2]\n",
      " ['seasonfrom' 2]\n",
      " ['oworoshoki' 2]\n",
      " ['icaseit' 2]\n",
      " ['urgentthere' 2]\n",
      " ['onshit' 2]\n",
      " ['unaddressed' 2]\n",
      " ['newsmornings' 2]\n",
      " ['quizze' 2]\n",
      " ['\\x89û÷it\\x89ûªs' 2]\n",
      " ['åç' 2]\n",
      " ['detouring' 2]\n",
      " ['enemity' 2]\n",
      " ['notexplaine' 2]\n",
      " ['northwestern\\x89û' 2]\n",
      " ['angelriveralib\\x89û' 2]\n",
      " ['help\\x89ûª' 2]\n",
      " ['techesback' 2]\n",
      " ['yazidi' 2]\n",
      " ['\\x89ââ' 2]\n",
      " ['softenza' 2]\n",
      " ['ushed' 2]\n",
      " ['alrasyid' 2]\n",
      " ['smantibatam' 2]\n",
      " ['^ag' 2]\n",
      " ['microsoft\\x89ûªs' 2]\n",
      " ['sms:<number' 2]\n",
      " ['walerga' 2]\n",
      " ['tlvfaces' 2]\n",
      " ['thebookclub' 2]\n",
      " [\"took'em\" 2]\n",
      " ['pletch\\x89ûªs' 2]\n",
      " ['\\x89û÷good' 2]\n",
      " ['number>-wheeler' 2]\n",
      " ['wdsu' 2]\n",
      " ['achimota' 2]\n",
      " ['for\\x89û' 2]\n",
      " ['eudrylantiqua' 2]\n",
      " ['pulwama' 2]\n",
      " ['pakistan\\x89ûªs' 2]\n",
      " ['stagefright' 2]\n",
      " ['p.o.p.e' 2]\n",
      " ['whitbourne' 2]\n",
      " ['autoinsurance' 2]\n",
      " ['user>.cbplawyers' 2]\n",
      " ['vabengal' 2]\n",
      " ['zss' 2]\n",
      " ['mcgsecure' 2]\n",
      " ['quivk' 2]\n",
      " ['user><hashtag' 2]\n",
      " ['skanndtyagi' 2]\n",
      " ['kosciusko' 2]\n",
      " ['easternoregon\\x89û' 2]\n",
      " ['i\\x89ûªd' 2]\n",
      " ['offr' 2]\n",
      " ['newsdict' 2]\n",
      " ['muzzamil' 2]\n",
      " ['liveleakfun' 2]\n",
      " ['forestservice' 2]\n",
      " ['dhsscitech' 2]\n",
      " ['buildings\\x89ûówe' 2]\n",
      " ['repeat><user' 2]\n",
      " ['so\\x89û' 2]\n",
      " ['ebolaåêcase' 2]\n",
      " ['lavenderpoetrycafe' 2]\n",
      " ['to\\x89û' 2]\n",
      " ['p\\x89û' 2]\n",
      " ['state\\x89û' 2]\n",
      " ['redeemeth' 2]\n",
      " ['wasn\\x89ûªt' 2]\n",
      " ['\\x89ûïmake' 2]\n",
      " ['suruc' 2]\n",
      " ['mukilteo' 2]\n",
      " ['duststorm' 2]\n",
      " ['||' 2]\n",
      " ['ifak' 2]\n",
      " ['smile>\\x89û' 2]\n",
      " ['ncjfcj' 2]\n",
      " ['allcaps>=' 2]\n",
      " ['stormchase' 2]\n",
      " ['renison' 2]\n",
      " ['have\\x89û' 2]\n",
      " ['informant\\x89ûªs' 2]\n",
      " [\"u're\" 2]\n",
      " [\"gov't\" 2]\n",
      " ['jamaicaplain' 2]\n",
      " ['inciweb' 2]\n",
      " ['swayback' 2]\n",
      " ['la\\x89û' 2]\n",
      " ['tonight\\x89ûªs' 2]\n",
      " ['pickerel' 2]\n",
      " ['faceåê(photos' 2]\n",
      " ['w\\x89û' 2]\n",
      " ['-<user' 2]\n",
      " ['schwarber' 2]\n",
      " ['disclos' 2]\n",
      " ['b\\x89û' 2]\n",
      " ['shizune' 2]\n",
      " ['number>-<number' 2]\n",
      " ['robotcoingame' 2]\n",
      " ['satoshis' 2]\n",
      " ['sebee' 2]\n",
      " ['hammondville' 2]\n",
      " ['are\\x89û' 2]\n",
      " ['feelingmanly' 2]\n",
      " ['mountain\\x89û' 2]\n",
      " ['notley' 2]\n",
      " ['pcps' 2]\n",
      " ['number><lolface' 2]\n",
      " ['thi\\x89û' 2]\n",
      " ['zabadani' 2]\n",
      " ['nankana' 2]\n",
      " ['airplaneåê(<number><number><number' 2]\n",
      " ['and\\x89û' 2]\n",
      " ['njenga' 2]\n",
      " ['kowing' 2]\n",
      " ['queenåê' 2]\n",
      " ['user><sadface' 2]\n",
      " ['horndale' 2]\n",
      " ['allcaps>:fyi' 2]\n",
      " ['fahlo:<hashtag' 2]\n",
      " ['japton' 2]\n",
      " ['woman\\x89ûªs' 2]\n",
      " ['carryi' 2]\n",
      " ['volcanoåêinåêrussia' 2]\n",
      " ['area\\x89û' 2]\n",
      " ['siskiyou' 2]\n",
      " ['aogashima' 2]\n",
      " ['insas' 2]\n",
      " ['amageddon' 2]\n",
      " ['amp;<hashtag' 2]\n",
      " ['chonce' 2]\n",
      " ['popularmmos' 2]\n",
      " ['minecraft-' 2]\n",
      " ['._.' 2]\n",
      " ['teslas' 2]\n",
      " ['petition!take' 2]\n",
      " ['die!please' 2]\n",
      " ['yycweather' 2]\n",
      " ['pakistan#news' 2]\n",
      " ['pakpattan' 2]\n",
      " ['\\x89û÷first\\x89ûª' 2]\n",
      " ['\\x89û÷second\\x89ûª' 2]\n",
      " ['u\\x89û' 2]\n",
      " ['ofclans' 2]\n",
      " ['lt;&lt' 2]\n",
      " ['ferguson\\x89ûªs' 2]\n",
      " ['florida\\x89û' 2]\n",
      " ['foxnew\\x89û' 2]\n",
      " ['allcaps>|' 2]\n",
      " ['allcaps><user' 2]\n",
      " ['thda' 2]\n",
      " ['number>(<number' 2]\n",
      " ['iclown' 2]\n",
      " ['m\\x89û' 2]\n",
      " ['wroug' 2]\n",
      " ['samaritans\\x89ûª' 2]\n",
      " ['hitchbot' 2]\n",
      " ['betraye' 2]\n",
      " ['magner' 2]\n",
      " ['wpri' 2]\n",
      " ['ushanka' 2]\n",
      " ['braininjury' 2]\n",
      " ['mothernaturenetwork' 2]\n",
      " ['nasahurricane' 2]\n",
      " ['raì¼l' 2]\n",
      " ['new\\x89û' 2]\n",
      " ['today\\x89ûªs' 2]\n",
      " ['on\\x89û' 2]\n",
      " ['\\x89ûïstretcher' 2]\n",
      " ['deck\\x89û\\x9d' 2]\n",
      " ['ca\\x89û' 2]\n",
      " ['jhaustin' 2]\n",
      " ['en\\x89û' 2]\n",
      " ['ll=' 2]\n",
      " ['matako_milk' 2]\n",
      " ['-a' 2]\n",
      " ['elong' 2]\n",
      " ['calgaryweather' 2]\n",
      " ['suruì¤' 2]\n",
      " ['magisters' 2]\n",
      " [\"\\x89û¢åê'demolition\" 2]\n",
      " ['allcaps>+++++' 2]\n",
      " ['pyrotechnic' 2]\n",
      " ['valleywx' 2]\n",
      " ['gaelite' 2]\n",
      " ['transcen' 2]\n",
      " [\"ol'head\" 2]\n",
      " ['gmmbc' 2]\n",
      " [\"\\x89û¢åê'nigeria\" 2]\n",
      " ['allcaps>?#tepco' 2]\n",
      " ['dmpl' 2]\n",
      " ['ihhen' 2]\n",
      " ['mì¼sica' 2]\n",
      " ['~<number>%' 2]\n",
      " ['kimery' 2]\n",
      " ['-popular' 2]\n",
      " ['finall\\x89û' 1]\n",
      " ['windstormåêinsurer' 1]\n",
      " ['riooooos' 1]\n",
      " ['whitewashes' 1]\n",
      " [\"user>'=blackcats\" 1]\n",
      " ['mart.sun.-mushroom' 1]\n",
      " ['foragesecret' 1]\n",
      " ['lisowski' 1]\n",
      " ['ktfounder' 1]\n",
      " ['hendrixonfire' 1]\n",
      " ['servicin' 1]\n",
      " ['jsdf' 1]\n",
      " ['splattershot' 1]\n",
      " ['-honey' 1]\n",
      " ['wsvr' 1]\n",
      " ['trailheads' 1]\n",
      " ['catalogue~' 1]\n",
      " ['farmr' 1]\n",
      " ['articals' 1]\n",
      " ['morels' 1]\n",
      " ['decision\\x89û' 1]\n",
      " ['ineedexposure' 1]\n",
      " ['cantwaittoplayinminneapolis' 1]\n",
      " ['splatoon' 1]\n",
      " ['splatling' 1]\n",
      " ['galv' 1]\n",
      " ['south\\x89û' 1]\n",
      " ['trend.\\x89û\\x9d' 1]\n",
      " ['syjexo' 1]\n",
      " ['\\x89ûïnumbers' 1]\n",
      " ['lancasteronline' 1]\n",
      " ['hearths' 1]\n",
      " ['lastingness' 1]\n",
      " ['ziuw' 1]\n",
      " ['astrologian' 1]\n",
      " ['number>-hours' 1]\n",
      " ['jiahahahha' 1]\n",
      " ['splatdown' 1]\n",
      " ['bookmobile' 1]\n",
      " ['kwwwkwwwk' 1]\n",
      " ['doessnt' 1]\n",
      " ['ojou' 1]\n",
      " ['b&amp;n' 1]\n",
      " ['fro\\x89û' 1]\n",
      " ['nightmarishly' 1]\n",
      " ['-legion' 1]\n",
      " ['wqow' 1]\n",
      " ['picthis' 1]\n",
      " ['yeyeulala' 1]\n",
      " ['hah-' 1]\n",
      " ['gbonyin' 1]\n",
      " ['int\\x89ûªl' 1]\n",
      " ['windriver' 1]\n",
      " ['andals' 1]\n",
      " ['pokì' 1]\n",
      " ['dismaying' 1]\n",
      " ['watchcbs' 1]\n",
      " ['dugway' 1]\n",
      " [\"user>'court\" 1]\n",
      " ['allcaps>&amp;wht' 1]\n",
      " ['allcaps>-from' 1]\n",
      " [\"us'g\" 1]\n",
      " ['look?<user' 1]\n",
      " ['atgames' 1]\n",
      " ['ashens' 1]\n",
      " ['cruisingblazing' 1]\n",
      " ['timedoes' 1]\n",
      " ['-rare' 1]\n",
      " ['åáåáåá' 1]\n",
      " ['åêcall' 1]\n",
      " ['\\x89û÷crying' 1]\n",
      " ['|live' 1]\n",
      " ['d&amp;d' 1]\n",
      " ['elong>-' 1]\n",
      " ['.go' 1]\n",
      " ['wehtwtvwlocal' 1]\n",
      " ['kzim' 1]\n",
      " ['industryinsights' 1]\n",
      " ['delaval' 1]\n",
      " ['texas\\x89ûªs' 1]\n",
      " ['gayåêman' 1]\n",
      " ['vì_a' 1]\n",
      " ['\\x89û÷assault' 1]\n",
      " ['accountability\\x89ûª' 1]\n",
      " ['her\\x89û' 1]]\n"
     ]
    }
   ],
   "source": [
    "oov_df = oov_df.sort_values(by='number',ascending=False)\n",
    "print(oov_df.head(500).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of tokens oov:  0.023790731762318187\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of tokens oov: \", \n",
    "      oov_df.number.sum() / (train['nlp'].apply(len).sum() + test['nlp'].apply(len).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate concatenated wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(doc):    \n",
    "    concat = numpy.empty((0, doc.vocab.vectors_length))\n",
    "    for token in doc:\n",
    "        concat = numpy.append(concat, [token.vector], axis = 0)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['wordvec_concat'] = train['nlp'].apply(lambda s: concatenate(s))\n",
    "test['wordvec_concat'] = test['nlp'].apply(lambda s: concatenate(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 25)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['wordvec_concat'][10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated averaged wordvecs using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train['cleaned_text'].append(test['cleaned_text'],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "vectorizer.fit_transform(sentences)\n",
    "idf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def get_tfidf_mean(doc):\n",
    "    avg = numpy.zeros(len(doc.vector))\n",
    "    n = 0\n",
    "    for token in doc:        \n",
    "        if not token.is_oov:\n",
    "            token_str = str(token).lower()\n",
    "            if token_str in idf:\n",
    "                avg += token.vector * idf[token_str]\n",
    "                n += 1\n",
    "    if n>0:\n",
    "        avg = avg/n\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['wordvec_tfidf'] = train['nlp'].apply(lambda s: get_tfidf_mean(s))\n",
    "test['wordvec_tfidf'] = test['nlp'].apply(lambda s: get_tfidf_mean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-0.4900269259053927, 1.0778943672776222, -1.7...\n",
       "1       [-3.9615797315325056, -1.5446132038320814, -2....\n",
       "2       [-2.1109436134045776, 3.079569337212227, -0.78...\n",
       "3       [-1.9814261879239763, 4.416366151400974, -3.53...\n",
       "4       [-2.43848686615626, 0.6247085710366567, 1.3067...\n",
       "                              ...                        \n",
       "7556    [-4.0677333739068775, 0.2842167301310433, 2.45...\n",
       "7557    [-0.4930134703301721, 0.1357929338183668, -0.9...\n",
       "7558    [-0.7093669772148132, 0.7391833066940308, 0.59...\n",
       "7559    [-1.9778278940602352, 1.721481098940498, -0.25...\n",
       "7560    [-1.0706927833909339, 0.13924438641829925, -0....\n",
       "Name: wordvec_tfidf, Length: 7561, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['wordvec_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.032318536, 0.12929907, -0.35238042, 0.04862...\n",
       "1       [-0.40937126, -0.32038173, -0.21201876, -0.004...\n",
       "2       [-0.17538336, 0.25031605, -0.11056641, -0.1157...\n",
       "3       [-0.062765546, 0.457585, -0.46744218, -0.26988...\n",
       "4       [-0.22135386, 0.082693174, 0.080948114, 0.0187...\n",
       "                              ...                        \n",
       "7556    [-0.41963834, -0.021819353, 0.30132917, 0.0265...\n",
       "7557    [0.027209358, -0.032375943, -0.13020572, -0.04...\n",
       "7558    [0.3659297, -0.16286746, -0.12082411, -0.52468...\n",
       "7559    [0.025096677, -0.031307478, -0.043185014, -0.1...\n",
       "7560    [0.099660195, -0.14457195, -0.1661434, -0.3547...\n",
       "Name: wordvec, Length: 7561, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['wordvec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To explore: use wordninja to cut up composite words/hashtags\n",
    "Several OOV words seem to be composites that could be cut up. Wordninja can do this. For now just leave as is, since it's only 4% of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['typhoon', 'devastated']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('typhoondevastated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mh', '370']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('mh370')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre', 'break']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('prebreak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate wordvecs for multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"wordvecs/glove.twitter.27B.25d\",\n",
    "          \"wordvecs/glove.twitter.27B.50d\",\n",
    "          \"wordvecs/glove.twitter.27B.100d\",\n",
    "          \"wordvecs/glove.twitter.27B.200d\",\n",
    "          \"en_core_web_lg\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model  wordvecs/glove.twitter.27B.25d\n",
      "Wordvec length  25\n",
      "Fraction of tokens oov:  1.9 %\n",
      "Loading model  wordvecs/glove.twitter.27B.50d\n",
      "Wordvec length  50\n",
      "Fraction of tokens oov:  1.9 %\n",
      "Loading model  wordvecs/glove.twitter.27B.100d\n",
      "Wordvec length  100\n",
      "Fraction of tokens oov:  1.9 %\n",
      "Loading model  wordvecs/glove.twitter.27B.200d\n",
      "Wordvec length  200\n",
      "Fraction of tokens oov:  1.9 %\n",
      "Loading model  en_core_web_lg\n",
      "Wordvec length  300\n",
      "Fraction of tokens oov:  3.3 %\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    model_name = m.split('/')[-1]\n",
    "    print(\"Loading model \",m)\n",
    "    nlp = spacy.load(m)\n",
    "    print(\"Wordvec length \",nlp.vocab.vectors_length)\n",
    "    if model_name.startswith('glove.'):\n",
    "        text_col = 'glove_cleaned_text'\n",
    "    else:\n",
    "        text_col = 'cleaned_text'\n",
    "    \n",
    "    train['nlp'] = train[text_col].apply(lambda s: nlp(s))\n",
    "    train['wordvec'] = train['nlp'].apply(lambda s: s.vector)\n",
    "    test['nlp'] = test[text_col].apply(lambda s: nlp(s))\n",
    "    test['wordvec'] = test['nlp'].apply(lambda s: s.vector)\n",
    "    \n",
    "    train['cleaned_text_keyword'] = (train['keyword'] + ' ' + train[text_col]).str.strip()\n",
    "    test['cleaned_text_keyword'] = (test['keyword'] + ' ' + test[text_col]).str.strip()\n",
    "    train['keyword_nlp'] = train['cleaned_text_keyword'].apply(lambda s:nlp(s))\n",
    "    train['keyword_wordvec'] = train['keyword_nlp'].apply(lambda s: s.vector)\n",
    "    test['keyword_nlp'] = test['cleaned_text_keyword'].apply(lambda s:nlp(s))\n",
    "    test['keyword_wordvec'] = test['keyword_nlp'].apply(lambda s: s.vector)\n",
    "    \n",
    "    oov_tokens = dict()\n",
    "    \n",
    "    train['nlp'].apply(lambda x: gather_tokens(oov_tokens,x))\n",
    "    oov_df = pd.DataFrame({'token':list(oov_tokens.keys()), 'number':list(oov_tokens.values())})\n",
    "    print(\"Fraction of tokens oov: \", round(100.0*\n",
    "      oov_df.number.sum() / train['nlp'].apply(len).sum(),1), '%')\n",
    "    \n",
    "    train['wordvec_concat'] = train['nlp'].apply(lambda s: concatenate(s))\n",
    "    test['wordvec_concat'] = test['nlp'].apply(lambda s: concatenate(s))\n",
    "    \n",
    "    train['wordvec_tfidf'] = train['nlp'].apply(lambda s: get_tfidf_mean(s))\n",
    "    test['wordvec_tfidf'] = test['nlp'].apply(lambda s: get_tfidf_mean(s))\n",
    "    \n",
    "    columns_to_save = ['id', 'wordvec', 'keyword_wordvec', 'wordvec_concat', 'wordvec_tfidf']\n",
    "    train[columns_to_save].to_pickle('train_wordvec_'+model_name+'.pickle')\n",
    "    test[columns_to_save].to_pickle('test_wordvec_'+model_name+'.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following wordvec data is produced:\n",
    "* wordvec: wordvec from the tweet (average of the wordvec of all words in the tweet)\n",
    "* keyword_wordvec: wordvec from keyword_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
