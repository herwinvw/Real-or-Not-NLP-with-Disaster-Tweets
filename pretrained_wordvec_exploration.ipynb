{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor.defines import Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_wv(tokens, wv):\n",
    "    vec_sum = numpy.zeros((1, wv.vector_size))\n",
    "    n = 0\n",
    "    for t in tokens:\n",
    "        if t in wv:\n",
    "            n+=1\n",
    "            vec_sum += wv[t]\n",
    "    return vec_sum/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_wv(tokens, wv):    \n",
    "    concat = numpy.empty((0, wv.vector_size))\n",
    "    for t in tokens:\n",
    "        if t in wv:\n",
    "            concat = numpy.append(concat, [wv[t]], axis = 0)\n",
    "        else:\n",
    "            concat = numpy.append(concat, numpy.zeros((1, wv.vector_size)),axis=0)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage(tokens, wv):\n",
    "    in_voc = tokens.apply(lambda x:sum([t in wv for t in x]))\n",
    "    number = tokens.apply(len)\n",
    "    return sum(in_voc)/sum(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordvecs from Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load('wordvecs/glove.twitter.27B.100d.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Preprocessing reimplemented from Ruby:\n",
    "https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from preprocessor.defines import Patterns\n",
    "\n",
    "# Different regex parts for smiley faces\n",
    "eyes = \"[8:=;]\"\n",
    "nose = \"['`\\-]?\"\n",
    "\n",
    "def split_hashtag(m):\n",
    "    hashtag = m.group()\n",
    "    hashtag_body = hashtag[1:]\n",
    "    if hashtag_body.upper() == hashtag_body:\n",
    "        result = f\"_hashtag_ {hashtag_body}\"\n",
    "    else:\n",
    "        result = \"_hashtag_ \"+(\" \".join([] + re.split(\"(?=[A-Z])\",hashtag_body))).strip()\n",
    "    return result\n",
    "            \n",
    "def glove_cleanup(tweet):\n",
    "    tweet = re.sub(Patterns.URL_PATTERN,'_url_',tweet)\n",
    "    tweet = re.sub(\"/\",\" / \",tweet)\n",
    "    tweet = re.sub(Patterns.MENTION_PATTERN,\"_user_\",tweet)\n",
    "    tweet = re.sub(f\"{eyes}{nose}[)d]+|[(d]+{nose}{eyes}\",\"_smile_\",tweet)\n",
    "    tweet = re.sub(f\"{eyes}{nose}p+\", \"_lolface_\",tweet)\n",
    "    tweet = re.sub(f\"{eyes}{nose}\\(+|\\)+{nose}{eyes}\", \"_sadface_\",tweet)\n",
    "    tweet = re.sub(f\"{eyes}{nose}[\\/|l*]\", \"_neutralface_\",tweet)\n",
    "    tweet = re.sub(\"<3\",\"_heart_\",tweet)\n",
    "    tweet = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\",\"_number_\",tweet)\n",
    "    tweet = re.sub(\"#\\S+\", split_hashtag, tweet)\n",
    "    tweet = re.sub(r'([!?.]){2,}',r'\\1 _repeat_', tweet)\n",
    "    tweet = re.sub(r'\\b(\\S*?)(\\S)\\2{2,}\\b', r'\\1\\2 _elong_',  tweet)\n",
    "    tweet = re.sub(r'(\\b[A-Z][A-Z]+\\b)',r'\\1 _allcaps_',tweet)\n",
    "    tweet = re.sub(r'\\s+',' ',tweet) #remove whitespace repetition\n",
    "    return tweet.lower()\n",
    "\n",
    "GLOVE_TWITTER_TOKENS = ['url', 'user', 'smile', 'lolface','sadface','neutralface',\n",
    "                 'heart','number','allcaps','hashtag','repeat','elong']\n",
    "\n",
    "def reset_glove_token(t):\n",
    "    for token in GLOVE_TWITTER_TOKENS:\n",
    "        t = t.replace(f'_{token}_',f'<{token}>')\n",
    "    return t\n",
    "        \n",
    "def reset_glove_tokens(tokens):\n",
    "    return [reset_glove_token(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "The glove Twitter wordvecs are tokenized using the Stanford tokenizer:\n",
    "https://nlp.stanford.edu/software/tokenizer.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.destructive import NLTKWordTokenizer #similar tokenizer in Python\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = NLTKWordTokenizer()\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, <hash...\n",
       "1          [forest, fire, near, la, ronge, sask., canada]\n",
       "2       [all, residents, asked, to, 'shelter, in, plac...\n",
       "3       [<number>, people, receive, <hashtag>, wildfir...\n",
       "4       [just, got, sent, this, photo, from, ruby, <ha...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [<user>, <user>, the, out, of, control, wild, ...\n",
       "7610    [m<number>, [, <number>, utc, <allcaps>, ], ?,...\n",
       "7611    [police, investigating, after, an, e-bike, col...\n",
       "7612    [the, latest, :, more, homes, razed, by, north...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train['text'].apply(glove_cleanup)\n",
    "tokens = text.apply(lambda x:tokenizer.tokenize(x))\n",
    "#tokens = text.apply(word_tokenize)\n",
    "tokens = tokens.apply(reset_glove_tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7613.000000\n",
       "mean       17.646920\n",
       "std         7.087759\n",
       "min         1.000000\n",
       "25%        12.000000\n",
       "50%        18.000000\n",
       "75%        23.000000\n",
       "max        56.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9464591428103554"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_coverage(tokens, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordvecs from Godin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load('wordvecs/word2vec_twitter_tokens.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Our, Deeds, are, the, Reason, of, this, #eart...\n",
       "1     [Forest, fire, near, La, Ronge, Sask, ., Canada]\n",
       "2    [All, residents, asked, to, ', shelter, in, pl...\n",
       "3    [13,000, people, receive, #wildfires, evacuati...\n",
       "4    [Just, got, sent, this, photo, from, Ruby, #Al...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = train['text'].apply(twokenize.tokenize)\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion https://t.co/gFJfgTodad\n",
      "['.', '_MENTION_', '#Bahrain', 'police', 'had', 'previously', 'died', 'in', 'a', 'road', 'accident', 'they', 'were', 'not', 'killed', 'by', 'explosion', '_URL_']\n",
      "I still have not heard Church Leaders of Kenya coming forward to comment on the accident issue and disciplinary measures#ArrestPastorNganga\n",
      "['I', 'still', 'have', 'not', 'heard', 'Church', 'Leaders', 'of', 'Kenya', 'coming', 'forward', 'to', 'comment', 'on', 'the', 'accident', 'issue', 'and', 'disciplinary', 'measures#ArrestPastorNganga']\n",
      "@afterShock_DeLo scuf ps live and the game... cya\n",
      "['_MENTION_', 'scuf', 'ps', 'live', 'and', 'the', 'game', '...', 'cya']\n",
      "'The man who can drive himself further once the effort gets painful is the man who will win.' \n",
      "Roger Bannister\n",
      "[\"'\", 'The', 'man', 'who', 'can', 'drive', 'himself', 'further', 'once', 'the', 'effort', 'gets', 'painful', 'is', 'the', 'man', 'who', 'will', 'win', \".'\", 'Roger', 'Bannister']\n",
      "320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/yNXnvVKCDA | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/weQPesENku\n",
      "['_NUMBER_', '[', 'IR', ']', 'ICEMOON', '[', 'AFTERSHOCK', ']', '|', '_URL_', '|', '_MENTION_', '|', '#Dubstep', '#TrapMusic', '#DnB', '#EDM', '#Dance', '#Ices\\x89Û_', '_URL_']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def replace_in_tokens(tokens, pattern, replacement):\n",
    "    return tokens.apply(lambda tok: [re.sub(pattern, replacement,t) for t in tok])\n",
    "\n",
    "tokens = replace_in_tokens(tokens, Patterns.MENTION_PATTERN, '_MENTION_')\n",
    "tokens = replace_in_tokens(tokens, Patterns.URL_PATTERN, '_URL_')\n",
    "tokens = replace_in_tokens(tokens, r'[0-9]+', '_NUMBER_')\n",
    "for i in range(100,105):\n",
    "    print(train['text'].iloc[i])\n",
    "    print(tokens.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7613.000000\n",
       "mean       16.341258\n",
       "std         6.396467\n",
       "min         1.000000\n",
       "25%        11.000000\n",
       "50%        16.000000\n",
       "75%        21.000000\n",
       "max        39.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3039345"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different hashtags : 1927\n",
      "Number : 3403\n",
      "#news        76\n",
      "#            73\n",
      "#hot         31\n",
      "#best        30\n",
      "#prebreak    30\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "hashtags = train['text'].str.lower().apply(lambda x: Patterns.HASHTAG_PATTERN.findall(x))\n",
    "name_set = set(chain.from_iterable(hashtags.values))\n",
    "name_list = list(chain.from_iterable(hashtags.values))\n",
    "print(\"Number of different hashtags :\", len(name_set))\n",
    "print(\"Number :\", len(name_list))\n",
    "names = pd.Series(0, index=list(name_set))\n",
    "for name in name_list:\n",
    "    names[name]+=1\n",
    "print(names.sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748751101968851"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([name in wv for name in name_list])/len(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6751427088738973"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([name in wv for name in name_set])/len(name_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = names.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>number</th>\n",
       "      <th>oov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#news</td>\n",
       "      <td>76</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#</td>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#hot</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#best</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#prebreak</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#nowplaying</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#islam</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#hiroshima</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#earthquake</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#gbbo</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  number    oov\n",
       "0        #news      76  False\n",
       "1            #      73  False\n",
       "2         #hot      31  False\n",
       "3        #best      30  False\n",
       "4    #prebreak      30   True\n",
       "5  #nowplaying      23  False\n",
       "6       #islam      23  False\n",
       "7   #hiroshima      22  False\n",
       "8  #earthquake      19  False\n",
       "9        #gbbo      18  False"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = names.reset_index(name='number')\n",
    "names['oov'] = names['index'].apply(lambda x:x not in wv)\n",
    "names.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9684259601626931"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_coverage(tokens, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wordvecs = tokens.apply(lambda x: get_average_wv(x, wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_wordvecs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_wordvecs = tokens.apply(lambda x: get_concat_wv(x, wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 400)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_wordvecs[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
