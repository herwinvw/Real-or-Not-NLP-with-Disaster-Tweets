{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional wordvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_train = pd.read_pickle('train_wordvec_glove.twitter.27B.50d.pickle')\n",
    "wordvec_test = pd.read_pickle('test_wordvec_glove.twitter.27B.50d.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>glove_cleaned_text</th>\n",
       "      <th>wordvec</th>\n",
       "      <th>keyword_wordvec</th>\n",
       "      <th>wordvec_concat</th>\n",
       "      <th>wordvec_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>our deeds are the reason of this &lt;hashtag&gt; ear...</td>\n",
       "      <td>[0.34491748, 0.22578713, 0.07250176, 0.0747826...</td>\n",
       "      <td>[0.34491748, 0.22578713, 0.07250176, 0.0747826...</td>\n",
       "      <td>[[0.15189999341964722, 0.042114000767469406, 0...</td>\n",
       "      <td>[1.5667167289980821, 1.2339818275400571, 0.247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>[0.012921251, -0.21105601, -0.035843372, -0.42...</td>\n",
       "      <td>[0.012921251, -0.21105601, -0.035843372, -0.42...</td>\n",
       "      <td>[[-0.18448999524116516, -0.9394599795341492, -...</td>\n",
       "      <td>[-0.3091522455215454, -0.7302671798637935, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[0.18257721, 0.20186086, -0.12769058, 0.055459...</td>\n",
       "      <td>[0.18257721, 0.20186086, -0.12769058, 0.055459...</td>\n",
       "      <td>[[0.3380799889564514, 0.24919000267982483, 0.2...</td>\n",
       "      <td>[1.0368728519163348, 2.0143982385369865, -1.50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>&lt;number&gt; people receive &lt;hashtag&gt; wildfires ev...</td>\n",
       "      <td>[0.40764716, 0.24536107, -0.17184022, -0.19602...</td>\n",
       "      <td>[0.40764716, 0.24536107, -0.17184022, -0.19602...</td>\n",
       "      <td>[[0.476610004901886, 0.20970000326633453, 0.33...</td>\n",
       "      <td>[2.8280542948179774, 1.9225198494063482, -2.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>just got sent this photo from ruby &lt;hashtag&gt; a...</td>\n",
       "      <td>[0.2010703, 0.15718287, 0.14506513, -0.1997651...</td>\n",
       "      <td>[0.2010703, 0.15718287, 0.14506513, -0.1997651...</td>\n",
       "      <td>[[0.07746600359678268, 0.37777000665664673, 0....</td>\n",
       "      <td>[1.232528381049633, 1.0207276116399204, 0.7743...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1       1             Forest fire near La Ronge Sask. Canada   \n",
       "2       1  All residents asked to 'shelter in place' are ...   \n",
       "3       1  people receive wildfires evacuation orders in ...   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                  glove_cleaned_text  \\\n",
       "0  our deeds are the reason of this <hashtag> ear...   \n",
       "1             forest fire near la ronge sask. canada   \n",
       "2  all residents asked to 'shelter in place' are ...   \n",
       "3  <number> people receive <hashtag> wildfires ev...   \n",
       "4  just got sent this photo from ruby <hashtag> a...   \n",
       "\n",
       "                                             wordvec  \\\n",
       "0  [0.34491748, 0.22578713, 0.07250176, 0.0747826...   \n",
       "1  [0.012921251, -0.21105601, -0.035843372, -0.42...   \n",
       "2  [0.18257721, 0.20186086, -0.12769058, 0.055459...   \n",
       "3  [0.40764716, 0.24536107, -0.17184022, -0.19602...   \n",
       "4  [0.2010703, 0.15718287, 0.14506513, -0.1997651...   \n",
       "\n",
       "                                     keyword_wordvec  \\\n",
       "0  [0.34491748, 0.22578713, 0.07250176, 0.0747826...   \n",
       "1  [0.012921251, -0.21105601, -0.035843372, -0.42...   \n",
       "2  [0.18257721, 0.20186086, -0.12769058, 0.055459...   \n",
       "3  [0.40764716, 0.24536107, -0.17184022, -0.19602...   \n",
       "4  [0.2010703, 0.15718287, 0.14506513, -0.1997651...   \n",
       "\n",
       "                                      wordvec_concat  \\\n",
       "0  [[0.15189999341964722, 0.042114000767469406, 0...   \n",
       "1  [[-0.18448999524116516, -0.9394599795341492, -...   \n",
       "2  [[0.3380799889564514, 0.24919000267982483, 0.2...   \n",
       "3  [[0.476610004901886, 0.20970000326633453, 0.33...   \n",
       "4  [[0.07746600359678268, 0.37777000665664673, 0....   \n",
       "\n",
       "                                       wordvec_tfidf  \n",
       "0  [1.5667167289980821, 1.2339818275400571, 0.247...  \n",
       "1  [-0.3091522455215454, -0.7302671798637935, -0....  \n",
       "2  [1.0368728519163348, 2.0143982385369865, -1.50...  \n",
       "3  [2.8280542948179774, 1.9225198494063482, -2.34...  \n",
       "4  [1.232528381049633, 1.0207276116399204, 0.7743...  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(wordvec_train, on=['id'])\n",
    "test = test.merge(wordvec_test, on=['id'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = max(train['wordvec_concat'].apply(lambda x: x.shape[0]).max(),\n",
    "                test['wordvec_concat'].apply(lambda x: x.shape[0]).max())\n",
    "max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_size = train['wordvec_concat'].iloc[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def get_X(df, col, wv_size):\n",
    "    X = concat = numpy.empty((0, max_words, wv_size))\n",
    "    for index, row in df.iterrows():\n",
    "        x = numpy.pad(row[col],((0,max_words - row[col].shape[0]),(0, 0)))\n",
    "        X = numpy.append(X, [x], axis=0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "X = get_X(train, 'wordvec_concat', wv_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7561, 109, 50)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model():\n",
    "    conv_model = tf.keras.Sequential([\\\n",
    "        tf.keras.layers.Dropout(0.4, input_shape=(max_words, wv_size)),          \n",
    "        tf.keras.layers.Conv1D(filters=6, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Dropout(0.3),  \n",
    "        tf.keras.layers.Conv1D(filters=8, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.2),                           \n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid'),\n",
    "    ])\n",
    "    return conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def __init__(self, train, validation):   \n",
    "        super(Metrics, self).__init__()\n",
    "        self.validation = validation    \n",
    "        self.train = train        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):        \n",
    "        self.val_f1s = []\n",
    "        self.train_f1s = []\n",
    "             \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_targ = self.validation[1]   \n",
    "        val_predict = (np.asarray(self.model.predict(self.validation[0]))).round()        \n",
    "        \n",
    "        train_targ = self.train[1]   \n",
    "        train_predict = (np.asarray(self.model.predict(self.train[0]))).round()   \n",
    "        \n",
    "        val_f1 = f1_score(val_targ, val_predict)\n",
    "        train_f1 = f1_score(train_targ, train_predict)\n",
    "        self.val_f1s.append(round(val_f1, 6))\n",
    "        self.train_f1s.append(round(train_f1, 6))\n",
    "        \n",
    "        print(f'— train_f1: {train_f1} — val_f1: {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Starting fold 1 ----\n",
      "Epoch 1/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.6993\n",
      "— train_f1: 0.6259373621526246 — val_f1: 0.6538124452234882\n",
      "Epoch 2/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.6071\n",
      "— train_f1: 0.7117539213689141 — val_f1: 0.7124394184168013\n",
      "Epoch 3/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.5606\n",
      "— train_f1: 0.7324516785350966 — val_f1: 0.732258064516129\n",
      "Epoch 4/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.5294\n",
      "— train_f1: 0.7463355048859934 — val_f1: 0.7390263367916999\n",
      "Epoch 5/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.5262\n",
      "— train_f1: 0.7510862818125388 — val_f1: 0.739591836734694\n",
      "Epoch 6/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.5148\n",
      "— train_f1: 0.7540305911533691 — val_f1: 0.7406199021207178\n",
      "Epoch 7/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.5023\n",
      "— train_f1: 0.7612850752338349 — val_f1: 0.752205292702486\n",
      "Epoch 8/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.5128\n",
      "— train_f1: 0.7628021302744776 — val_f1: 0.7526358475263584\n",
      "Epoch 9/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4953\n",
      "— train_f1: 0.7548278757346768 — val_f1: 0.7441860465116279\n",
      "Epoch 10/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4952\n",
      "— train_f1: 0.7726358148893361 — val_f1: 0.7605409705648369\n",
      "Epoch 11/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4786\n",
      "— train_f1: 0.7469405594405595 — val_f1: 0.7340241796200345\n",
      "Epoch 12/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4900\n",
      "— train_f1: 0.7583438952260245 — val_f1: 0.745\n",
      "Epoch 13/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4774\n",
      "— train_f1: 0.7645580226225388 — val_f1: 0.7524752475247524\n",
      "Epoch 14/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4803\n",
      "— train_f1: 0.7698004525817733 — val_f1: 0.7607461476074615\n",
      "Epoch 15/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4731\n",
      "— train_f1: 0.7678795483061481 — val_f1: 0.754748142031379\n",
      "Epoch 16/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4751\n",
      "— train_f1: 0.7671060891399873 — val_f1: 0.7580511973575558\n",
      "Epoch 17/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4845\n",
      "— train_f1: 0.7622094263169119 — val_f1: 0.7468460891505466\n",
      "Epoch 18/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4704\n",
      "— train_f1: 0.7608977125593439 — val_f1: 0.7463952502120441\n",
      "Epoch 19/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4635\n",
      "— train_f1: 0.7756701030927835 — val_f1: 0.7703464947622884\n",
      "Epoch 20/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4813\n",
      "— train_f1: 0.7662671232876713 — val_f1: 0.7529411764705883\n",
      "Epoch 21/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4716\n",
      "— train_f1: 0.7640882794086136 — val_f1: 0.7489433643279797\n",
      "Epoch 22/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4702\n",
      "— train_f1: 0.761659144637868 — val_f1: 0.7495769881556685\n",
      "Epoch 23/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4570\n",
      "— train_f1: 0.7621306879447919 — val_f1: 0.7491582491582491\n",
      "Epoch 24/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4592\n",
      "— train_f1: 0.7453444020641687 — val_f1: 0.7344434706397897\n",
      "Epoch 25/25\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.4693\n",
      "— train_f1: 0.7631864553939657 — val_f1: 0.7497879558948262\n",
      "---- Starting fold 2 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.6879\n",
      "— train_f1: 0.6588983050847457 — val_f1: 0.6387959866220735\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.6109\n",
      "— train_f1: 0.7311577311577313 — val_f1: 0.711760184473482\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5516\n",
      "— train_f1: 0.7546383945475199 — val_f1: 0.7331821617535904\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5273\n",
      "— train_f1: 0.7362452750944981 — val_f1: 0.7266553480475382\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4958\n",
      "— train_f1: 0.7325959661678594 — val_f1: 0.7166521360069749\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5067\n",
      "— train_f1: 0.7571865443425075 — val_f1: 0.7387687188019967\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4917\n",
      "— train_f1: 0.7587764319441593 — val_f1: 0.7378151260504202\n",
      "Epoch 8/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4867\n",
      "— train_f1: 0.7235280949182895 — val_f1: 0.7038703870387037\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4985\n",
      "— train_f1: 0.7648966355897853 — val_f1: 0.7446102819237147\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4848\n",
      "— train_f1: 0.7587492234417064 — val_f1: 0.7381756756756757\n",
      "Epoch 11/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4913\n",
      "— train_f1: 0.7606571012684551 — val_f1: 0.7351443123938878\n",
      "Epoch 12/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4850\n",
      "— train_f1: 0.765045342126958 — val_f1: 0.7468460891505466\n",
      "Epoch 13/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4651\n",
      "— train_f1: 0.7726809378185525 — val_f1: 0.7495854063018244\n",
      "Epoch 14/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4710\n",
      "— train_f1: 0.7700205338809035 — val_f1: 0.7504159733777038\n",
      "Epoch 15/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4814\n",
      "— train_f1: 0.7664818237831176 — val_f1: 0.746044962531224\n",
      "Epoch 16/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4707\n",
      "— train_f1: 0.7654063087528723 — val_f1: 0.7421808960270498\n",
      "Epoch 17/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4780\n",
      "— train_f1: 0.7643714466203412 — val_f1: 0.7450980392156863\n",
      "Epoch 18/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4677\n",
      "— train_f1: 0.7792362768496421 — val_f1: 0.7540453074433656\n",
      "Epoch 19/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4802\n",
      "— train_f1: 0.7639974646101838 — val_f1: 0.7446626814688301\n",
      "Epoch 20/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4737\n",
      "— train_f1: 0.7695507487520798 — val_f1: 0.7466442953020135\n",
      "Epoch 21/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4744\n",
      "— train_f1: 0.7632933104631219 — val_f1: 0.7352941176470588\n",
      "Epoch 22/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4979\n",
      "— train_f1: 0.7729628861704334 — val_f1: 0.7518796992481204\n",
      "Epoch 23/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4661\n",
      "— train_f1: 0.7716075156576201 — val_f1: 0.7512605042016807\n",
      "Epoch 24/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4594\n",
      "— train_f1: 0.7771405161818927 — val_f1: 0.7502061005770817\n",
      "Epoch 25/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4660\n",
      "— train_f1: 0.7795227411788701 — val_f1: 0.7545304777594728\n",
      "---- Starting fold 3 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.6916\n",
      "— train_f1: 0.6548042704626335 — val_f1: 0.6688524590163935\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.6024\n",
      "— train_f1: 0.6761692650334076 — val_f1: 0.6912280701754386\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5412\n",
      "— train_f1: 0.7334027055150886 — val_f1: 0.7431421446384041\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5252\n",
      "— train_f1: 0.7468896593922089 — val_f1: 0.7524590163934427\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5050\n",
      "— train_f1: 0.7589498806682577 — val_f1: 0.7587859424920127\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4915\n",
      "— train_f1: 0.7593123209169055 — val_f1: 0.7539094650205761\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4907\n",
      "— train_f1: 0.73923393204934 — val_f1: 0.7361111111111113\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4816\n",
      "— train_f1: 0.7678355501813785 — val_f1: 0.7595141700404858\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4950\n",
      "— train_f1: 0.7719652595435266 — val_f1: 0.7625201938610663\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5083\n",
      "— train_f1: 0.7719226856561546 — val_f1: 0.7603574329813161\n",
      "Epoch 11/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4897\n",
      "— train_f1: 0.7742461287693562 — val_f1: 0.7662337662337663\n",
      "Epoch 12/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4788\n",
      "— train_f1: 0.7705358990275192 — val_f1: 0.7597027250206442\n",
      "Epoch 13/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4802\n",
      "— train_f1: 0.7768595041322314 — val_f1: 0.7691069991954949\n",
      "Epoch 14/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4874\n",
      "— train_f1: 0.7810392701309005 — val_f1: 0.7677725118483412\n",
      "Epoch 15/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4758\n",
      "— train_f1: 0.7634522051065626 — val_f1: 0.7472527472527473\n",
      "Epoch 16/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4621\n",
      "— train_f1: 0.7551152272237777 — val_f1: 0.742127659574468\n",
      "Epoch 17/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4757\n",
      "— train_f1: 0.7781583719524481 — val_f1: 0.7586206896551724\n",
      "Epoch 18/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4796\n",
      "— train_f1: 0.7718496989827694 — val_f1: 0.7506255212677231\n",
      "Epoch 19/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4773\n",
      "— train_f1: 0.7508189561039529 — val_f1: 0.7378472222222223\n",
      "Epoch 20/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4660\n",
      "— train_f1: 0.7522336020919589 — val_f1: 0.738115816767502\n",
      "Epoch 21/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4671\n",
      "— train_f1: 0.7599480968858132 — val_f1: 0.7401032702237521\n",
      "Epoch 22/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4649\n",
      "— train_f1: 0.766532854426368 — val_f1: 0.7485281749369218\n",
      "Epoch 23/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4631\n",
      "— train_f1: 0.7601031814273431 — val_f1: 0.7442650807136788\n",
      "Epoch 24/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4687\n",
      "— train_f1: 0.7721413721413721 — val_f1: 0.750207468879668\n",
      "Epoch 25/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4737\n",
      "— train_f1: 0.7490570224095849 — val_f1: 0.7341549295774649\n",
      "---- Starting fold 4 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.7239\n",
      "— train_f1: 0.02406919894697255 — val_f1: 0.01210287443267776\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.6722\n",
      "— train_f1: 0.414789139225881 — val_f1: 0.4177071509648127\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5992\n",
      "— train_f1: 0.7124735729386893 — val_f1: 0.7027942421676545\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5399\n",
      "— train_f1: 0.7309666381522668 — val_f1: 0.7144075021312872\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5048\n",
      "— train_f1: 0.7174153573799513 — val_f1: 0.7066079295154185\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5056\n",
      "— train_f1: 0.7250608272506084 — val_f1: 0.7159190853122251\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4941\n",
      "— train_f1: 0.7430036317026277 — val_f1: 0.7393526405451448\n",
      "Epoch 8/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4949\n",
      "— train_f1: 0.7320804195804196 — val_f1: 0.7302688638334778\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4889\n",
      "— train_f1: 0.7262619524127195 — val_f1: 0.7146714671467146\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4922\n",
      "— train_f1: 0.7387387387387389 — val_f1: 0.736098852603707\n",
      "Epoch 11/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4841\n",
      "— train_f1: 0.763447559709242 — val_f1: 0.7489643744821871\n",
      "Epoch 12/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4712\n",
      "— train_f1: 0.7558313717098225 — val_f1: 0.7440273037542663\n",
      "Epoch 13/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4806\n",
      "— train_f1: 0.7578542423594786 — val_f1: 0.7463706233988044\n",
      "Epoch 14/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4668\n",
      "— train_f1: 0.7604255319148936 — val_f1: 0.7448630136986302\n",
      "Epoch 15/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4716\n",
      "— train_f1: 0.7544161999138304 — val_f1: 0.7443868739205526\n",
      "Epoch 16/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4805\n",
      "— train_f1: 0.7449428320140722 — val_f1: 0.7304964539007093\n",
      "Epoch 17/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4737\n",
      "— train_f1: 0.7700290577002906 — val_f1: 0.7489643744821871\n",
      "Epoch 18/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4865\n",
      "— train_f1: 0.7608695652173912 — val_f1: 0.7442258340461934\n",
      "Epoch 19/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4722\n",
      "— train_f1: 0.7621266680787969 — val_f1: 0.7446988973706531\n",
      "Epoch 20/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4700\n",
      "— train_f1: 0.7395484015202325 — val_f1: 0.7233273056057866\n",
      "Epoch 21/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4679\n",
      "— train_f1: 0.7589304812834223 — val_f1: 0.7408041060735671\n",
      "Epoch 22/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4719\n",
      "— train_f1: 0.75390625 — val_f1: 0.7458515283842795\n",
      "Epoch 23/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4687\n",
      "— train_f1: 0.7633425473102275 — val_f1: 0.7489288774635818\n",
      "Epoch 24/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4820\n",
      "— train_f1: 0.7530594405594404 — val_f1: 0.7389770723104057\n",
      "Epoch 25/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4709\n",
      "— train_f1: 0.7747895709299939 — val_f1: 0.7567127746135069\n",
      "---- Starting fold 5 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.7190\n",
      "— train_f1: 0.5755884494054841 — val_f1: 0.5341246290801186\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.6160\n",
      "— train_f1: 0.721576354679803 — val_f1: 0.72\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5596\n",
      "— train_f1: 0.7223753976670202 — val_f1: 0.716852010265184\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5440\n",
      "— train_f1: 0.7155153387773119 — val_f1: 0.7020335985853225\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5253\n",
      "— train_f1: 0.7330220713073006 — val_f1: 0.7320819112627986\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5114\n",
      "— train_f1: 0.73832562053008 — val_f1: 0.7455621301775147\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.5019\n",
      "— train_f1: 0.7218908769604595 — val_f1: 0.7149161518093557\n",
      "Epoch 8/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4974\n",
      "— train_f1: 0.7318132464712269 — val_f1: 0.7241079199303743\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4973\n",
      "— train_f1: 0.724130290272546 — val_f1: 0.7213403880070547\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4881\n",
      "— train_f1: 0.738115816767502 — val_f1: 0.7446259673258815\n",
      "Epoch 11/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4969\n",
      "— train_f1: 0.737444933920705 — val_f1: 0.7400346620450606\n",
      "Epoch 12/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4867\n",
      "— train_f1: 0.711121365943701 — val_f1: 0.7098540145985403\n",
      "Epoch 13/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4863\n",
      "— train_f1: 0.7255877034358048 — val_f1: 0.730186999109528\n",
      "Epoch 14/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4989\n",
      "— train_f1: 0.7330976579761379 — val_f1: 0.7371179039301311\n",
      "Epoch 15/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4868\n",
      "— train_f1: 0.7472432432432432 — val_f1: 0.7504302925989674\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4860\n",
      "— train_f1: 0.7333481844508799 — val_f1: 0.742556917688266\n",
      "Epoch 17/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4823\n",
      "— train_f1: 0.7453794303109372 — val_f1: 0.751931330472103\n",
      "Epoch 18/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4833\n",
      "— train_f1: 0.7363941769316911 — val_f1: 0.7387068201948627\n",
      "Epoch 19/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4677\n",
      "— train_f1: 0.7495674740484429 — val_f1: 0.7510692899914458\n",
      "Epoch 20/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4827\n",
      "— train_f1: 0.7306908267270668 — val_f1: 0.7357142857142858\n",
      "Epoch 21/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4905\n",
      "— train_f1: 0.7423407538020719 — val_f1: 0.7462946817785527\n",
      "Epoch 22/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4882\n",
      "— train_f1: 0.7667952100669778 — val_f1: 0.7676113360323886\n",
      "Epoch 23/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4828\n",
      "— train_f1: 0.7289550891446626 — val_f1: 0.7323187108325873\n",
      "Epoch 24/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4792\n",
      "— train_f1: 0.7416777629826897 — val_f1: 0.7421052631578948\n",
      "Epoch 25/25\n",
      "190/190 [==============================] - 1s 4ms/step - loss: 0.4780\n",
      "— train_f1: 0.7320942883046238 — val_f1: 0.7383512544802868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(5, random_state=42, shuffle=True)\n",
    "metrics = []\n",
    "for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(X=X, y=y)):\n",
    "    print('---- Starting fold %d ----'%(k_fold+1))\n",
    "    \n",
    "    x_train, y_train = X[tr_inds], y[tr_inds]\n",
    "    x_val, y_val = X[val_inds], y[val_inds]\n",
    "    conv_model = get_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    conv_model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=[])    \n",
    "    m = Metrics(train=(x_train,y_train), validation=(x_val, y_val))\n",
    "    conv_model.fit(x=x_train, y=y_train, batch_size=32, epochs=25, \n",
    "               callbacks=[m])\n",
    "    metrics.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for m in metrics:\n",
    "    scores.append(m.val_f1s[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.749788, 0.75453, 0.734155, 0.756713, 0.738351]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7467074"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginally better as averaging?? Train model on all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.6993\n",
      "Epoch 2/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.6141\n",
      "Epoch 3/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.5525\n",
      "Epoch 4/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.5207\n",
      "Epoch 5/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.5154\n",
      "Epoch 6/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4857\n",
      "Epoch 7/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4884\n",
      "Epoch 8/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4941\n",
      "Epoch 9/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4910\n",
      "Epoch 10/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4761\n",
      "Epoch 11/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4819\n",
      "Epoch 12/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4867\n",
      "Epoch 13/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4703\n",
      "Epoch 14/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4789\n",
      "Epoch 15/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4778\n",
      "Epoch 16/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4701\n",
      "Epoch 17/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4784\n",
      "Epoch 18/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4786\n",
      "Epoch 19/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4762\n",
      "Epoch 20/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4610\n",
      "Epoch 21/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4664\n",
      "Epoch 22/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4771\n",
      "Epoch 23/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4695\n",
      "Epoch 24/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4618\n",
      "Epoch 25/25\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc7701a1820>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = get_model()\n",
    "conv_model.compile(loss='binary_crossentropy', optimizer= \"adam\")    \n",
    "conv_model.fit(x=X, y=y, batch_size=32, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_X(test, 'wordvec_concat', wv_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = conv_model.predict(X_test)\n",
    "pred = pred.flatten().round()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"id\":test['id'], \"target\":pred.flatten().round().astype(int)})\n",
    "submission.to_csv('conv_net.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try RNNs instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_rnn_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(16, dropout=0.1, input_shape=(max_words,wv_size)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_20 (SimpleRNN)    (None, 16)                1072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,153\n",
      "Trainable params: 1,121\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_rnn_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Starting fold 1 ----\n",
      "Epoch 1/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.7185\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 2/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6820\n",
      "— train_f1: 0.0 — val_f1: 0.0030816640986132513\n",
      "Epoch 3/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6840\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 4/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6866\n",
      "— train_f1: 0.0007716049382716049 — val_f1: 0.0\n",
      "Epoch 5/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6808\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 6/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6826\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 7/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6870\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 8/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6864\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 9/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6867\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 10/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6850\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 11/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6834\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 12/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6798\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 13/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6800\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 14/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6845\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 15/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6833\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 16/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6825\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 17/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6857\n",
      "— train_f1: 0.47105075053609724 — val_f1: 0.4642604387827317\n",
      "Epoch 18/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6831\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 19/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6836\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 20/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6826\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 21/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6812\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 22/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6825\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 23/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6847\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 24/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6792\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 25/25\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6841\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "---- Starting fold 2 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 3s 12ms/step - loss: 0.7241\n",
      "— train_f1: 0.6016705820934481 — val_f1: 0.6105695228322217\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6804\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6662\n",
      "— train_f1: 0.6055230125523012 — val_f1: 0.6095751854349292\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6567\n",
      "— train_f1: 0.6186666666666666 — val_f1: 0.6195652173913044\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6626\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6602\n",
      "— train_f1: 0.5999768438115086 — val_f1: 0.6002779064381658\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6856\n",
      "— train_f1: 0.18935269207501512 — val_f1: 0.16038882138517618\n",
      "Epoch 8/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6861\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6846\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6850\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 11/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6847\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 12/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6836\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 13/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6832\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 14/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6835\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 15/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6822\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 16/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6892\n",
      "— train_f1: 0.34876756507717116 — val_f1: 0.35661764705882354\n",
      "Epoch 17/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6831\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 18/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6827\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 19/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6847\n",
      "— train_f1: 0.19303423848878395 — val_f1: 0.2033492822966507\n",
      "Epoch 20/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6849\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 21/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6779\n",
      "— train_f1: 0.22589206723680333 — val_f1: 0.22802850356294538\n",
      "Epoch 22/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6855\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 23/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6849\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 24/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6872\n",
      "— train_f1: 0.07689643228264634 — val_f1: 0.08044382801664356\n",
      "Epoch 25/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6819\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "---- Starting fold 3 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 3s 12ms/step - loss: 0.7297\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6809\n",
      "— train_f1: 0.007674597083653108 — val_f1: 0.00920245398773006\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6824\n",
      "— train_f1: 0.017497147204260172 — val_f1: 0.00916030534351145\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6857\n",
      "— train_f1: 0.011489850631941783 — val_f1: 0.009188361408882082\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6799\n",
      "— train_f1: 0.030052592036063114 — val_f1: 0.0121765601217656\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6824\n",
      "— train_f1: 0.012213740458015269 — val_f1: 0.009188361408882082\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6811\n",
      "— train_f1: 0.003850596842510589 — val_f1: 0.0030816640986132513\n",
      "Epoch 8/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6847\n",
      "— train_f1: 0.006153846153846154 — val_f1: 0.0030816640986132513\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6787\n",
      "— train_f1: 0.6003939288610821 — val_f1: 0.5999072786277238\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6813\n",
      "— train_f1: 0.013740458015267175 — val_f1: 0.009174311926605503\n",
      "Epoch 11/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6823\n",
      "— train_f1: 0.008448540706605223 — val_f1: 0.00920245398773006\n",
      "Epoch 12/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6823\n",
      "— train_f1: 0.6004404775704184 — val_f1: 0.6004640371229698\n",
      "Epoch 13/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6805\n",
      "— train_f1: 0.005386687187379762 — val_f1: 0.0030816640986132513\n",
      "Epoch 14/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6826\n",
      "— train_f1: 0.5996747211895911 — val_f1: 0.5985130111524163\n",
      "Epoch 15/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6774\n",
      "— train_f1: 0.02046229632436529 — val_f1: 0.015174506828528073\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6832\n",
      "— train_f1: 0.006917755572636434 — val_f1: 0.00920245398773006\n",
      "Epoch 17/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6815\n",
      "— train_f1: 0.009984639016897083 — val_f1: 0.009216589861751152\n",
      "Epoch 18/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6812\n",
      "— train_f1: 0.21846072467601166 — val_f1: 0.2191780821917808\n",
      "Epoch 19/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6864\n",
      "— train_f1: 0.0015420200462606015 — val_f1: 0.0\n",
      "Epoch 20/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6833\n",
      "— train_f1: 0.0015420200462606015 — val_f1: 0.0\n",
      "Epoch 21/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6803\n",
      "— train_f1: 0.0015420200462606015 — val_f1: 0.0\n",
      "Epoch 22/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6842\n",
      "— train_f1: 0.0030816640986132513 — val_f1: 0.0\n",
      "Epoch 23/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6847\n",
      "— train_f1: 0.0030816640986132513 — val_f1: 0.0\n",
      "Epoch 24/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6845\n",
      "— train_f1: 0.0038476337052712585 — val_f1: 0.003072196620583717\n",
      "Epoch 25/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6836\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "---- Starting fold 4 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 3s 12ms/step - loss: 0.6999\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6821\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6818\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6830\n",
      "— train_f1: 0.0015426147319706905 — val_f1: 0.003072196620583717\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6840\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.5996292863762743\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6829\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6829\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 8/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6821\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.6002779064381658\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6834\n",
      "— train_f1: 0.6001158077591199 — val_f1: 0.5996292863762743\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6831\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 11/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6844\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.5996292863762743\n",
      "Epoch 12/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6826\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 13/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6819\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.5996292863762743\n",
      "Epoch 14/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6825\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 15/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6830\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.5996292863762743\n",
      "Epoch 16/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6829\n",
      "— train_f1: 0.0015426147319706905 — val_f1: 0.003076923076923077\n",
      "Epoch 17/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6806\n",
      "— train_f1: 0.0015426147319706905 — val_f1: 0.003076923076923077\n",
      "Epoch 18/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6814\n",
      "— train_f1: 0.6000695329702167 — val_f1: 0.5998142989786444\n",
      "Epoch 19/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6814\n",
      "— train_f1: 0.0015426147319706905 — val_f1: 0.003076923076923077\n",
      "Epoch 20/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6802\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 21/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6838\n",
      "— train_f1: 0.0007716049382716049 — val_f1: 0.0\n",
      "Epoch 22/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6818\n",
      "— train_f1: 0.6002317497103129 — val_f1: 0.6001855287569574\n",
      "Epoch 23/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6826\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 24/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6833\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 25/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6834\n",
      "— train_f1: 0.0030828516377649326 — val_f1: 0.0061349693251533735\n",
      "---- Starting fold 5 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 3s 12ms/step - loss: 0.7083\n",
      "— train_f1: 0.010687022900763359 — val_f1: 0.015267175572519085\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6792\n",
      "— train_f1: 0.014437689969604862 — val_f1: 0.006153846153846153\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6727\n",
      "— train_f1: 0.62257806244996 — val_f1: 0.6062893081761006\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6633\n",
      "— train_f1: 0.6221528048939217 — val_f1: 0.6113671274961597\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6503\n",
      "— train_f1: 0.0007710100231303007 — val_f1: 0.0030816640986132513\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6487\n",
      "— train_f1: 0.5317345462063262 — val_f1: 0.5178423236514523\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6520\n",
      "— train_f1: 0.19180576631259483 — val_f1: 0.1875\n",
      "Epoch 8/25\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.6847\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6862\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6869\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[14] = 66734 is not in [0, 1512)\n\t [[{{node GatherV2}}]]\n\t [[IteratorGetNext]] [Op:__inference_predict_function_1171959]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-8f3f3d9e09a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     model.fit(x=x_train, y=y_train, batch_size=32, epochs=25, \n\u001b[0m\u001b[1;32m     14\u001b[0m                callbacks=[m])\n\u001b[1;32m     15\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-b09a0b3c923b>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mval_targ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mval_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_targ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[14] = 66734 is not in [0, 1512)\n\t [[{{node GatherV2}}]]\n\t [[IteratorGetNext]] [Op:__inference_predict_function_1171959]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(5, random_state=42, shuffle=True)\n",
    "metrics = []\n",
    "for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(X=X, y=y)):\n",
    "    print('---- Starting fold %d ----'%(k_fold+1))\n",
    "    \n",
    "    x_train, y_train = X[tr_inds], y[tr_inds]\n",
    "    x_val, y_val = X[val_inds], y[val_inds]\n",
    "    model = get_rnn_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=[])    \n",
    "    m = Metrics(train=(x_train,y_train), validation=(x_val, y_val))\n",
    "    model.fit(x=x_train, y=y_train, batch_size=32, epochs=25, \n",
    "               callbacks=[m])\n",
    "    metrics.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for m in metrics:\n",
    "    scores.append(m.val_f1s[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_gru_model():\n",
    "    return tf.keras.Sequential([\n",
    "        #tf.keras.layers.GRU(8, dropout=0.35, input_shape=(max_words,wv_size)),\n",
    "        tf.keras.layers.GRU(8, dropout=0.3, input_shape=(max_words,wv_size)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_40 (GRU)                 (None, 8)                 1440      \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_gru_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Starting fold 1 ----\n",
      "Epoch 1/25\n",
      "189/189 [==============================] - 4s 14ms/step - loss: 0.6887\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 2/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6815\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 3/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6833\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 4/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6834\n",
      "— train_f1: 0.6000463177396943 — val_f1: 0.6\n",
      "Epoch 5/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6820\n",
      "— train_f1: 0.6000231669176417 — val_f1: 0.6002779064381658\n",
      "Epoch 6/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6830\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 7/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6836\n",
      "— train_f1: 0.6000231669176417 — val_f1: 0.6002779064381658\n",
      "Epoch 8/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6822\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.6002779064381658\n",
      "Epoch 9/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6850\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 10/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6841\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 11/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6842\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 12/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6855\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 13/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6804\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 14/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6830\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.6002779064381658\n",
      "Epoch 15/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6833\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 16/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6827\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 17/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6838\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 18/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6850\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 19/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6809\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 20/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6803\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 21/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6835\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 22/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6827\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.6002779064381658\n",
      "Epoch 23/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6858\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.6002779064381658\n",
      "Epoch 24/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6816\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 25/25\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.6835\n",
      "— train_f1: 0.6001853138753764 — val_f1: 0.6002779064381658\n",
      "---- Starting fold 2 ----\n",
      "Epoch 1/25\n",
      "190/190 [==============================] - 5s 23ms/step - loss: 0.6871\n",
      "— train_f1: 0.5999768438115086 — val_f1: 0.6005560704355886\n",
      "Epoch 2/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6829\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 3/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6807\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 4/25\n",
      "190/190 [==============================] - 4s 22ms/step - loss: 0.6856\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 5/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6838\n",
      "— train_f1: 0.6001158077591199 — val_f1: 0.6005560704355886\n",
      "Epoch 6/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6810\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 7/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6836\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 8/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6804\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 9/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6825\n",
      "— train_f1: 0.5999768438115086 — val_f1: 0.6005560704355886\n",
      "Epoch 10/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6776\n",
      "— train_f1: 0.6001158077591199 — val_f1: 0.6005560704355886\n",
      "Epoch 11/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6822\n",
      "— train_f1: 0.6001158077591199 — val_f1: 0.6005560704355886\n",
      "Epoch 12/25\n",
      "190/190 [==============================] - 4s 23ms/step - loss: 0.6853\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 13/25\n",
      "190/190 [==============================] - 4s 21ms/step - loss: 0.6822\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 14/25\n",
      "190/190 [==============================] - 4s 21ms/step - loss: 0.6793\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 15/25\n",
      "190/190 [==============================] - 4s 21ms/step - loss: 0.6834\n",
      "— train_f1: 0.6001158077591199 — val_f1: 0.6005560704355886\n",
      "Epoch 16/25\n",
      "190/190 [==============================] - 4s 21ms/step - loss: 0.6829\n",
      "— train_f1: 0.5998377564028277 — val_f1: 0.6005560704355886\n",
      "Epoch 17/25\n",
      "190/190 [==============================] - 4s 21ms/step - loss: 0.6792\n",
      "— train_f1: 0.0 — val_f1: 0.0\n",
      "Epoch 18/25\n",
      " 28/190 [===>..........................] - ETA: 3s - loss: 0.6833"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-3cd54e940352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     model.fit(x=x_train, y=y_train, batch_size=32, epochs=25, \n\u001b[0m\u001b[1;32m     14\u001b[0m                callbacks=[m])\n\u001b[1;32m     15\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(5, random_state=42, shuffle=True)\n",
    "metrics = []\n",
    "for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(X=X, y=y)):\n",
    "    print('---- Starting fold %d ----'%(k_fold+1))\n",
    "    \n",
    "    x_train, y_train = X[tr_inds], y[tr_inds]\n",
    "    x_val, y_val = X[val_inds], y[val_inds]\n",
    "    model = get_gru_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=[])    \n",
    "    m = Metrics(train=(x_train,y_train), validation=(x_val, y_val))\n",
    "    model.fit(x=x_train, y=y_train, batch_size=32, epochs=25, \n",
    "               callbacks=[m])\n",
    "    metrics.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for m in metrics:\n",
    "    scores.append(m.val_f1s[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = get_gru_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "gru_model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=[])    \n",
    "gru_model.fit(x=X, y=y, batch_size=32, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gru_model.predict(X_test)\n",
    "pred = pred.flatten().round()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"id\":test['id'], \"target\":pred.flatten().round().astype(int)})\n",
    "submission.to_csv('gru.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_lstm_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(8, dropout=0.35, input_shape=(max_words,200)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lstm_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(5, random_state=42, shuffle=True)\n",
    "metrics = []\n",
    "for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(X=X, y=y)):\n",
    "    print('---- Starting fold %d ----'%(k_fold+1))\n",
    "    \n",
    "    x_train, y_train = X[tr_inds], y[tr_inds]\n",
    "    x_val, y_val = X[val_inds], y[val_inds]\n",
    "    model = get_lstm_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=[])    \n",
    "    m = Metrics(train=(x_train,y_train), validation=(x_val, y_val))\n",
    "    model.fit(x=x_train, y=y_train, batch_size=32, epochs=25, \n",
    "               callbacks=[m])\n",
    "    metrics.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for m in metrics:\n",
    "    scores.append(m.val_f1s[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
