{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional wordvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_train = pd.read_pickle('train_wordvec.pickle')\n",
    "wordvec_test = pd.read_pickle('test_wordvec.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>wordvec</th>\n",
       "      <th>keyword_wordvec</th>\n",
       "      <th>wordvec_concat</th>\n",
       "      <th>wordvec_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[-0.26623327, 0.05843069, -0.1404636, -0.05265...</td>\n",
       "      <td>[-0.26623327, 0.05843069, -0.1404636, -0.05265...</td>\n",
       "      <td>[[-0.2820900082588196, 0.1519400030374527, -0....</td>\n",
       "      <td>[-2.0410312242232838, 0.1577752003302941, -0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[-0.025449565, 0.031005142, -0.15566371, -0.23...</td>\n",
       "      <td>[-0.025449565, 0.031005142, -0.15566371, -0.23...</td>\n",
       "      <td>[[0.3039900064468384, 0.20476000010967255, -0....</td>\n",
       "      <td>[-0.27185601989428204, 0.2042857458194097, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[0.0059339865, 0.016337818, -0.105279535, -0.0...</td>\n",
       "      <td>[0.0059339865, 0.016337818, -0.105279535, -0.0...</td>\n",
       "      <td>[[0.00997759960591793, -0.20995000004768372, -...</td>\n",
       "      <td>[0.07528745450756767, 0.11175614595413208, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>[-0.18147185, 0.20731743, 0.014147284, -0.2182...</td>\n",
       "      <td>[-0.18147185, 0.20731743, 0.014147284, -0.2182...</td>\n",
       "      <td>[[-0.19686000049114227, 0.1157900020480156, -0...</td>\n",
       "      <td>[-1.3403782035623277, 1.2000715562275477, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[-0.06394094, -0.01423019, 0.0063574947, 0.071...</td>\n",
       "      <td>[-0.06394094, -0.01423019, 0.0063574947, 0.071...</td>\n",
       "      <td>[[-0.02556299977004528, 0.444240003824234, -0....</td>\n",
       "      <td>[-0.7245167245467504, -0.364056259393692, 0.52...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1       1             Forest fire near La Ronge Sask. Canada   \n",
       "2       1  All residents asked to 'shelter in place' are ...   \n",
       "3       1  people receive wildfires evacuation orders in ...   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                             wordvec  \\\n",
       "0  [-0.26623327, 0.05843069, -0.1404636, -0.05265...   \n",
       "1  [-0.025449565, 0.031005142, -0.15566371, -0.23...   \n",
       "2  [0.0059339865, 0.016337818, -0.105279535, -0.0...   \n",
       "3  [-0.18147185, 0.20731743, 0.014147284, -0.2182...   \n",
       "4  [-0.06394094, -0.01423019, 0.0063574947, 0.071...   \n",
       "\n",
       "                                     keyword_wordvec  \\\n",
       "0  [-0.26623327, 0.05843069, -0.1404636, -0.05265...   \n",
       "1  [-0.025449565, 0.031005142, -0.15566371, -0.23...   \n",
       "2  [0.0059339865, 0.016337818, -0.105279535, -0.0...   \n",
       "3  [-0.18147185, 0.20731743, 0.014147284, -0.2182...   \n",
       "4  [-0.06394094, -0.01423019, 0.0063574947, 0.071...   \n",
       "\n",
       "                                      wordvec_concat  \\\n",
       "0  [[-0.2820900082588196, 0.1519400030374527, -0....   \n",
       "1  [[0.3039900064468384, 0.20476000010967255, -0....   \n",
       "2  [[0.00997759960591793, -0.20995000004768372, -...   \n",
       "3  [[-0.19686000049114227, 0.1157900020480156, -0...   \n",
       "4  [[-0.02556299977004528, 0.444240003824234, -0....   \n",
       "\n",
       "                                       wordvec_tfidf  \n",
       "0  [-2.0410312242232838, 0.1577752003302941, -0.8...  \n",
       "1  [-0.27185601989428204, 0.2042857458194097, -1....  \n",
       "2  [0.07528745450756767, 0.11175614595413208, -0....  \n",
       "3  [-1.3403782035623277, 1.2000715562275477, 0.11...  \n",
       "4  [-0.7245167245467504, -0.364056259393692, 0.52...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(wordvec_train, on=['id'])\n",
    "test = test.merge(wordvec_test, on=['id'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = max(train['wordvec_concat'].apply(lambda x: x.shape[0]).max(),\n",
    "                test['wordvec_concat'].apply(lambda x: x.shape[0]).max())\n",
    "max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def get_X(df, col):\n",
    "    X = concat = numpy.empty((0, max_words, 300))\n",
    "    for index, row in df.iterrows():\n",
    "        x = numpy.pad(row[col],((0,max_words - row[col].shape[0]),(0, 0)))\n",
    "        X = numpy.append(X, [x], axis=0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "X = get_X(train, 'wordvec_concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7561, 33, 300)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model():\n",
    "    conv_model = tf.keras.Sequential([\\\n",
    "        tf.keras.layers.Dropout(0.4, input_shape=(max_words,300)),          \n",
    "        tf.keras.layers.Conv1D(filters=6, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Dropout(0.3),  \n",
    "        tf.keras.layers.Conv1D(filters=8, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.2),                           \n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid'),\n",
    "    ])\n",
    "    return conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def __init__(self, train, validation):   \n",
    "        super(Metrics, self).__init__()\n",
    "        self.validation = validation    \n",
    "        self.train = train        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):        \n",
    "        self.val_f1s = []\n",
    "        self.train_f1s = []\n",
    "             \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_targ = self.validation[1]   \n",
    "        val_predict = (np.asarray(self.model.predict(self.validation[0]))).round()        \n",
    "        \n",
    "        train_targ = self.train[1]   \n",
    "        train_predict = (np.asarray(self.model.predict(self.train[0]))).round()   \n",
    "        \n",
    "        val_f1 = f1_score(val_targ, val_predict)\n",
    "        train_f1 = f1_score(train_targ, train_predict)\n",
    "        self.val_f1s.append(round(val_f1, 6))\n",
    "        self.train_f1s.append(round(train_f1, 6))\n",
    "        \n",
    "        print(f'— train_f1: {train_f1} — val_f1: {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Starting fold 1 ----\n",
      "Epoch 1/100\n",
      "186/189 [============================>.] - ETA: 0s - loss: 0.5181— train_f1: 0.7958222404259677 — val_f1: 0.7596153846153846\n",
      "189/189 [==============================] - 4s 20ms/step - loss: 0.5163\n",
      "Epoch 2/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.4028— train_f1: 0.8401639344262295 — val_f1: 0.7768069896743447\n",
      "189/189 [==============================] - 5s 24ms/step - loss: 0.4028\n",
      "Epoch 3/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.3503— train_f1: 0.838279569892473 — val_f1: 0.748709122203098\n",
      "189/189 [==============================] - 5s 25ms/step - loss: 0.3503\n",
      "Epoch 4/100\n",
      "  3/189 [..............................] - ETA: 3s - loss: 0.2512"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(5, random_state=42, shuffle=True)\n",
    "metrics = []\n",
    "for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(X=X, y=y)):\n",
    "    print('---- Starting fold %d ----'%(k_fold+1))\n",
    "    \n",
    "    x_train, y_train = X[tr_inds], y[tr_inds]\n",
    "    x_val, y_val = X[val_inds], y[val_inds]\n",
    "    conv_model = get_model()\n",
    "    conv_model.compile(loss='binary_crossentropy', optimizer= \"adam\", metrics=[])    \n",
    "    m = Metrics(train=(x_train,y_train), validation=(x_val, y_val))\n",
    "    conv_model.fit(x=x_train, y=y_train, batch_size=32, epochs=25, \n",
    "               callbacks=[m])\n",
    "    metrics.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for m in metrics:\n",
    "    scores.append(m.val_f1s[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.767461, 0.748264, 0.769231, 0.75616, 0.782824]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764788"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginally better as averaging?? Train model on all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.6483\n",
      "Epoch 2/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.5194\n",
      "Epoch 3/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4946\n",
      "Epoch 4/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4700\n",
      "Epoch 5/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4622\n",
      "Epoch 6/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4576\n",
      "Epoch 7/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4531\n",
      "Epoch 8/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4162\n",
      "Epoch 9/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4315\n",
      "Epoch 10/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4332\n",
      "Epoch 11/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4262\n",
      "Epoch 12/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4302\n",
      "Epoch 13/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4385\n",
      "Epoch 14/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4139\n",
      "Epoch 15/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4146\n",
      "Epoch 16/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4061\n",
      "Epoch 17/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4128\n",
      "Epoch 18/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4246\n",
      "Epoch 19/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4162\n",
      "Epoch 20/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4012\n",
      "Epoch 21/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.3995\n",
      "Epoch 22/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4041\n",
      "Epoch 23/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4007\n",
      "Epoch 24/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4049\n",
      "Epoch 25/25\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.4028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8f98527880>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = get_model()\n",
    "conv_model.compile(loss='binary_crossentropy', optimizer= \"adam\")    \n",
    "conv_model.fit(x=X, y=y, batch_size=32, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_X(test, 'wordvec_concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 1., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = conv_model.predict(X_test)\n",
    "pred = pred.flatten().round()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"id\":test['id'], \"target\":pred.flatten().round().astype(int)})\n",
    "submission.to_csv('conv_net.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
